{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bb37386",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "- **Purpose:** Environment setup and loading/validating feature-engineered dataset for fraud model training  \n",
    "- **Author:** Devbrew LLC  \n",
    "- **Last Updated:** October 23, 2025  \n",
    "- **Status:** In progress  \n",
    "- **License:** Apache 2.0 (Code) | Non-commercial (Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9021a209",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Dataset License Notice\n",
    "\n",
    "This notebook uses the **IEEE-CIS Fraud Detection dataset** from Kaggle.\n",
    "\n",
    "**Dataset License:** Non-commercial research use only  \n",
    "- You must download the dataset yourself from [Kaggle IEEE-CIS Competition](https://www.kaggle.com/c/ieee-fraud-detection)  \n",
    "- You must accept the competition rules before downloading  \n",
    "- Cannot be used for commercial purposes  \n",
    "- Cannot redistribute the raw dataset\n",
    "\n",
    "**Setup Instructions:** See [`../data_catalog/README.md`](../data_catalog/README.md) for download instructions.\n",
    "\n",
    "**Code License:** This notebook's code is licensed under Apache 2.0 (open source).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f244dd",
   "metadata": {},
   "source": [
    "## Notebook Configuration\n",
    "\n",
    "### Environment Setup\n",
    "\n",
    "We configure the Python environment with standardized settings, import required libraries, and set a fixed random seed for reproducibility. This ensures consistent results across runs and enables reliable experimentation.\n",
    "\n",
    "These settings establish the foundation for all model training operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86ad0d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment configured successfully\n",
      "pandas: 2.3.3\n",
      "numpy: 2.3.3\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from pathlib import Path\n",
    "import json\n",
    "import hashlib\n",
    "from typing import Dict, Any, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.float_format\", '{:.2f}'.format)\n",
    "\n",
    "# Plotting configuration\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "plt.rcParams[\"font.size\"] = 10\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"Environment configured successfully\")\n",
    "print(f\"pandas: {pd.__version__}\")\n",
    "print(f\"numpy: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded98098",
   "metadata": {},
   "source": [
    "### Path Configuration\n",
    "\n",
    "We define the project directory structure and validate that required processed data from feature engineering exists. The validation ensures we have the necessary inputs before proceeding with training.\n",
    "\n",
    "This configuration pattern ensures we can locate all required data artifacts from previous pipeline stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8231f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artifact Availability Check:\n",
      " - train_features.parquet: Found\n",
      " - feature_engineering_metadata.json: Found\n",
      "\n",
      "All required artifacts are available\n"
     ]
    }
   ],
   "source": [
    "# Project paths\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "DATA_DIR = PROJECT_ROOT / \"data_catalog\"\n",
    "IEEE_CIS_DIR = DATA_DIR / \"ieee-fraud\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "NOTEBOOKS_DIR = PROJECT_ROOT / \"notebooks\"\n",
    "\n",
    "# Ensure processed directory exists\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Expected artifacts\n",
    "FEATURES_PATH = PROCESSED_DIR / \"train_features.parquet\"\n",
    "FE_METADATA_PATH = PROCESSED_DIR / \"feature_engineering_metadata.json\"\n",
    "\n",
    "def validate_required_artifacts():\n",
    "    \"\"\"Validate that required artifacts exist before training.\"\"\"\n",
    "    path_status = {\n",
    "        'train_features.parquet': FEATURES_PATH.exists(),\n",
    "        'feature_engineering_metadata.json': FE_METADATA_PATH.exists()\n",
    "    }\n",
    "    print(\"Artifact Availability Check:\")\n",
    "    for name, exists in path_status.items():\n",
    "        status = \"Found\" if exists else \"Missing\"\n",
    "        print(f\" - {name}: {status}\")\n",
    "\n",
    "    all_exist = all(path_status.values())\n",
    "\n",
    "    if not all_exist:\n",
    "        print(\"\\n[WARNING] Some artifacts are missing; ensure feature engineering completed successfully\")\n",
    "    else:\n",
    "        print(\"\\nAll required artifacts are available\")\n",
    "\n",
    "artifact_status = validate_required_artifacts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97438de6",
   "metadata": {},
   "source": [
    "## Load Features & Data Manifest\n",
    "\n",
    "We load the feature-engineered dataset and validate integrity against recorded metadata. We also create a simple data manifest to document:\n",
    "- shape\n",
    "- feature count\n",
    "- missing values\n",
    "- target distribution (`isFraud`)\n",
    "- memory footprint\n",
    "- file hash (for reproducibility)\n",
    "\n",
    "### Validation Checklist\n",
    "- Verify shape matches metadata (590,540 Ã— 432)\n",
    "- Confirm zero missing values\n",
    "- Check target distribution (~3.5% fraud rate)\n",
    "- Validate `TransactionDT` exists for time-based split\n",
    "- Validate identifiers (`TransactionID`) and target (`isFraud`) are present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f80cc3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Feature-Engineered Data...\n",
      "Loaded features: 590,540 rows x 432 columns\n",
      "Memory usage: 2.04 GB\n",
      "\n",
      "Target Distribution:\n",
      "isFraud\n",
      "0    569877\n",
      "1     20663\n",
      "Name: count, dtype: int64\n",
      "Fraud rate: 3.50%\n",
      "\n",
      "Shape validation vs metadata: PASS\n",
      " - (Expected 590,540 x 432 columns, got 590,540 x 432 columns)\n",
      "\n",
      "Total missing values: 0\n",
      "Has TransactionDT: Yes\n",
      "Has TransactionID: Yes\n",
      "\n",
      "Dtype Summary:\n",
      " - float64: 394\n",
      " - object: 29\n",
      " - int64: 9\n",
      "\n",
      "Training data manifest saved to: /Users/joekariuki/Documents/Research/Projects/devbrew-payments-fraud-sanctions/data_catalog/processed/training_data_manifest.json\n"
     ]
    }
   ],
   "source": [
    "def file_sha256(path: Path, chunk_size: int = 2**20) -> Optional[str]:\n",
    "    \"\"\"Computer SHA-256 hash of a file; returns None if file missing\"\"\"\n",
    "    if not path.exists():\n",
    "        return None\n",
    "    h = hashlib.sha256()\n",
    "    with path.open(\"rb\") as f:\n",
    "        while True:\n",
    "            chunk = f.read(chunk_size)\n",
    "            if not chunk:\n",
    "                break\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def build_data_manifest(df: pd.DataFrame, file_path: Path, fe_meta: Optional[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"Create a manifest capturing data properties for reproducibility.\"\"\"\n",
    "    manifest: Dict[str, Any] = {\n",
    "        \"generated_at\": pd.Timestamp.now().isoformat(),\n",
    "        \"source_file\": str(file_path),\n",
    "        \"source_hash_sha256\": file_sha256(file_path),\n",
    "        \"rows\": int(df.shape[0]),\n",
    "        \"columns\": int(df.shape[1]),\n",
    "        \"memory_gb\": float(df.memory_usage().sum() / 1e9),\n",
    "        \"dtypes_summary\": df.dtypes.astype(str).value_counts().to_dict(),\n",
    "        \"null_values_total\": int(df.isna().sum().sum()),\n",
    "        \"columns_with_nulls\": df.columns[df.isna().any()].tolist(),\n",
    "        \"target\": \"isFraud\",\n",
    "        \"target_distribution\": df[\"isFraud\"].value_counts(dropna=False).to_dict() if \"isFraud\" in df.columns else {},\n",
    "        \"target_rate\": float(df[\"isFraud\"].mean()) if \"isFraud\" in df.columns else None,\n",
    "        \"has_transactiondt\": bool(\"TransactionDT\" in df.columns),\n",
    "        \"has_transactionid\": bool(\"TransactionID\" in df.columns),\n",
    "        \"random_state\": RANDOM_STATE,\n",
    "    }\n",
    "\n",
    "    if fe_meta:\n",
    "        manifest[\"feature_engineering_metadata\"] = {\n",
    "            \"total_features_expected\": fe_meta.get(\"total_features\"),\n",
    "            \"dataset_shape_expected\": fe_meta.get(\"dataset_shape\"),\n",
    "            \"engineering_date\": fe_meta.get(\"engineering_date\"),\n",
    "        }\n",
    "\n",
    "    return manifest\n",
    "\n",
    "# Load feature engineering metadata if available\n",
    "fe_meta = None\n",
    "if FE_METADATA_PATH.exists():\n",
    "    with open(FE_METADATA_PATH, \"r\") as f:\n",
    "        fe_meta = json.load(f)\n",
    "\n",
    "print(\"Loading Feature-Engineered Data...\")\n",
    "if not FEATURES_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Missing features file: {FEATURES_PATH}\")\n",
    "\n",
    "df = pd.read_parquet(FEATURES_PATH)\n",
    "print(f\"Loaded features: {df.shape[0]:,} rows x {df.shape[1]:,} columns\")\n",
    "print(f\"Memory usage: {df.memory_usage().sum() / 1e9:.2f} GB\")\n",
    "\n",
    "# Basic target info\n",
    "if \"isFraud\" in df.columns:\n",
    "    print(\"\\nTarget Distribution:\")\n",
    "    print(pd.Series(df[\"isFraud\"]).value_counts())\n",
    "    print(f\"Fraud rate: {df['isFraud'].mean() * 100:.2f}%\")\n",
    "else:\n",
    "    print(\"\\n[WARNING] 'isFraud' target column not found in features dataset\")\n",
    "\n",
    "# Data check vs metadata\n",
    "if fe_meta and \"dataset_shape\" in fe_meta:\n",
    "    expected_rows, expected_cols = fe_meta[\"dataset_shape\"]\n",
    "    ok_shape = (df.shape[0] == expected_rows) and (df.shape[1] == expected_cols)\n",
    "    print(f\"\\nShape validation vs metadata: {'PASS' if ok_shape else 'FAIL'}\")\n",
    "    print(f\" - (Expected {expected_rows:,} x {expected_cols:,} columns, got {df.shape[0]:,} x {df.shape[1]:,} columns)\")\n",
    "else:\n",
    "    print(\"\\n[WARNING] Feature engineering metadata not availables for shape validation\")\n",
    "\n",
    "# Nulls\n",
    "total_nulls = int(df.isnull().sum().sum())\n",
    "print(f\"\\nTotal missing values: {total_nulls:,}\")\n",
    "\n",
    "# Key column presence\n",
    "print(f\"Has TransactionDT: {'Yes' if 'TransactionDT' in df.columns else 'No'}\")\n",
    "print(f\"Has TransactionID: {'Yes' if 'TransactionID' in df.columns else 'No'}\")\n",
    "    \n",
    "\n",
    "# Dtypes quick summary\n",
    "dtype_counts = df.dtypes.astype(str).value_counts()\n",
    "print(\"\\nDtype Summary:\")\n",
    "for dtype, count in dtype_counts.items():\n",
    "    print(f\" - {dtype}: {count}\")\n",
    "\n",
    "# Build and save manifest\n",
    "manifest = build_data_manifest(df, FEATURES_PATH, fe_meta)\n",
    "\n",
    "# Save manifest to data catalog\n",
    "MANIFEST_PATH = PROCESSED_DIR / \"training_data_manifest.json\"\n",
    "MANIFEST_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(MANIFEST_PATH, \"w\") as f:\n",
    "    json.dump(manifest, f, indent=4)\n",
    "print(f\"\\nTraining data manifest saved to: {MANIFEST_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
