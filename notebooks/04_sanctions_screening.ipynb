{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sanctions Screening\n",
        "\n",
        "- **Purpose:** OFAC sanctions screening with fuzzy name matching for fraud detection pipeline  \n",
        "- **Author:** Devbrew LLC  \n",
        "- **Last Updated:** November 4, 2025  \n",
        "- **Status:** In progress  \n",
        "- **License:** Apache 2.0 (Code) | Public Domain (OFAC Data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Dataset License Notice\n",
        "\n",
        "This notebook uses **OFAC Sanctions Lists** (SDN and Consolidated) from the U.S. Department of the Treasury.\n",
        "\n",
        "**Dataset License:** Public Domain  \n",
        "- OFAC sanctions data is publicly available from [OFAC Sanctions List Search](https://sanctionslist.ofac.treas.gov/Home)  \n",
        "- Data can be freely used, redistributed, and incorporated into commercial systems  \n",
        "- Updates are published regularly; production systems should refresh data periodically  \n",
        "\n",
        "**Setup Instructions:** See [`../data_catalog/README.md`](../data_catalog/README.md) for download instructions.\n",
        "\n",
        "**Code License:** This notebook's code is licensed under Apache 2.0 (open source).\n",
        "\n",
        "**Disclaimer:** This is a research demonstration. Production sanctions screening requires broader list coverage (EU, UN, UK HMT), legal review, and compliance with local regulations.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notebook Configuration\n",
        "\n",
        "### Environment Setup\n",
        "\n",
        "We configure the Python environment with standardized settings, import required libraries for text processing and fuzzy matching, and set a fixed random seed for reproducibility. This ensures consistent results across runs and enables reliable experimentation.\n",
        "\n",
        "These settings establish the foundation for all sanctions screening operations, including name normalization, tokenization, and similarity scoring."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment configured successfully\n",
            "pandas: 2.3.3\n",
            "numpy: 2.3.3\n",
            "rapidfuzz: 3.14.1\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "from pathlib import Path\n",
        "import json\n",
        "import hashlib\n",
        "import unicodedata\n",
        "import re\n",
        "from typing import Dict, Any, Optional, List, Tuple\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import rapidfuzz as rf\n",
        "from rapidfuzz import fuzz, process\n",
        "\n",
        "# Configuration\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "pd.set_option(\"display.max_columns\", 100)\n",
        "pd.set_option(\"display.max_rows\", 100)\n",
        "pd.set_option(\"display.float_format\", '{:.2f}'.format)\n",
        "\n",
        "# Plotting configuration\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
        "plt.rcParams[\"font.size\"] = 10\n",
        "\n",
        "# Reproducibility\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "\n",
        "print(\"Environment configured successfully\")\n",
        "print(f\"pandas: {pd.__version__}\")\n",
        "print(f\"numpy: {np.__version__}\")\n",
        "print(f\"rapidfuzz: {rf.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Path Configuration\n",
        "\n",
        "We define the project directory structure and validate that OFAC data files exist before proceeding. The validation ensures we have the necessary sanctions lists for screening operations.\n",
        "\n",
        "This configuration pattern ensures we can locate all required data artifacts and provides clear feedback if prerequisites are missing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OFAC Data Availability Check:\n",
            " - SDN Primary              : Found\n",
            " - SDN Alternate            : Found\n",
            " - SDN Address              : Found\n",
            " - Consolidated Primary     : Found\n",
            " - Consolidated Alternate   : Found\n",
            " - Consolidated Address     : Found\n",
            "\n",
            "All required OFAC data files are available\n"
          ]
        }
      ],
      "source": [
        "# Project paths\n",
        "PROJECT_ROOT = Path.cwd().parent\n",
        "DATA_DIR = PROJECT_ROOT / \"data_catalog\"\n",
        "OFAC_DIR = DATA_DIR / \"ofac\"\n",
        "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
        "MODELS_DIR = PROJECT_ROOT / \"packages\" / \"models\"\n",
        "\n",
        "# Ensure output directories exist\n",
        "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
        "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Expected OFAC data files\n",
        "OFAC_FILES = {\n",
        "    'SDN Primary': OFAC_DIR / 'sdn' / 'sdn.csv',\n",
        "    'SDN Alternate': OFAC_DIR / 'sdn' / 'alt.csv',\n",
        "    'SDN Address': OFAC_DIR / 'sdn' / 'add.csv',\n",
        "    'Consolidated Primary': OFAC_DIR / 'consolidated' / 'cons_prim.csv',\n",
        "    'Consolidated Alternate': OFAC_DIR / 'consolidated' / 'cons_alt.csv',\n",
        "    'Consolidated Address': OFAC_DIR / 'consolidated' / 'cons_add.csv',\n",
        "}\n",
        "\n",
        "def validate_required_data():\n",
        "    \"\"\"Validate that OFAC sanctions data files exist.\"\"\"\n",
        "    print(\"OFAC Data Availability Check:\")\n",
        "    \n",
        "    all_exist = True\n",
        "    for name, path in OFAC_FILES.items():\n",
        "        exists = path.exists()\n",
        "        status = \"Found\" if exists else \"Missing\"\n",
        "        print(f\" - {name:25s}: {status}\")\n",
        "        if not exists:\n",
        "            all_exist = False\n",
        "    \n",
        "    if not all_exist:\n",
        "        print(\"\\n[WARNING] Some OFAC files are missing; see data_catalog/README.md for instructions\")\n",
        "    else:\n",
        "        print(\"\\nAll required OFAC data files are available\")\n",
        "    \n",
        "    return all_exist\n",
        "\n",
        "data_available = validate_required_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load & Normalize OFAC Datasets\n",
        "\n",
        "We load OFAC sanctions lists (SDN and Consolidated) and apply comprehensive text normalization to enable robust fuzzy matching. This step is critical for handling variations in how names appear across different systems and languages.\n",
        "\n",
        "Our normalization strategy addresses several common challenges in sanctions screening:\n",
        "- **Unicode variations**: Convert to canonical form (NFKC) to handle different encodings\n",
        "- **Accent marks**: Strip diacritics to match \"José\" with \"Jose\"\n",
        "- **Case sensitivity**: Lowercase everything for case-insensitive matching\n",
        "- **Punctuation**: Standardize hyphens, remove quotes that don't affect identity\n",
        "- **Whitespace**: Collapse multiple spaces to single space\n",
        "\n",
        "This preprocessing ensures we can match names reliably even when they're formatted differently in transaction data versus sanctions lists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing text normalization:\n",
            "\n",
            "  'José María O'Brien' → 'jose maria obrien'\n",
            "  'AL-QAIDA' → 'al-qaida'\n",
            "  'Société Générale' → 'societe generale'\n",
            "  '中国工商银行' → ''\n",
            "  '  Multiple   Spaces  ' → 'multiple spaces'\n",
            "  'UPPER-case-MiXeD' → 'upper-case-mixed'\n"
          ]
        }
      ],
      "source": [
        "def normalize_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Normalize text for robust fuzzy matching.\n",
        "    \n",
        "    Applies NFKC normalization, lowercasing, accent stripping,\n",
        "    punctuation canonicalization, and whitespace collapse.\n",
        "    \n",
        "    Note: Non-Latin scripts (Chinese, Arabic, Cyrillic) are stripped\n",
        "    because OFAC sanctions lists use romanized names. For example:\n",
        "    - \"中国工商银行\" → \"\" (empty)\n",
        "    - \"INDUSTRIAL AND COMMERCIAL BANK OF CHINA\" → \"industrial and commercial bank of china\"\n",
        "    \n",
        "    Args:\n",
        "        text: Raw text string to normalize\n",
        "        \n",
        "    Returns:\n",
        "        Normalized text string suitable for fuzzy matching.\n",
        "        Returns empty string if input contains only non-Latin characters.\n",
        "        \n",
        "    Examples:\n",
        "        >>> normalize_text(\"José María O'Brien\")\n",
        "        'jose maria obrien'\n",
        "        \n",
        "        >>> normalize_text(\"AL-QAIDA\")\n",
        "        'al qaida'\n",
        "        \n",
        "        >>> normalize_text(\"中国工商银行\")\n",
        "        ''\n",
        "    \"\"\"\n",
        "    if not text or pd.isna(text):\n",
        "        return \"\"\n",
        "    \n",
        "    # Convert to string if not already\n",
        "    text = str(text)\n",
        "    \n",
        "    # Unicode normalization (canonical composition)\n",
        "    text = unicodedata.normalize(\"NFKC\", text)\n",
        "    \n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Strip accent marks (diacritics)\n",
        "    # Decompose characters, then filter out combining marks\n",
        "    text = ''.join(\n",
        "        char for char in unicodedata.normalize(\"NFD\", text)\n",
        "        if unicodedata.category(char) != 'Mn'\n",
        "    )\n",
        "    \n",
        "    # Remove quotes (single and double)\n",
        "    text = re.sub(r\"['\\\"]\", \"\", text)\n",
        "    \n",
        "    # Replace non-alphanumeric (except space and hyphen) with space\n",
        "    # Note: This strips non-Latin scripts (Chinese, Arabic, Cyrillic, etc.)\n",
        "    # OFAC lists use romanized names, so this is intentional behavior\n",
        "    text = re.sub(r\"[^a-z0-9\\s-]\", \" \", text)\n",
        "    \n",
        "    # Collapse multiple spaces to single space\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    \n",
        "    # Strip leading/trailing whitespace\n",
        "    text = text.strip()\n",
        "    \n",
        "    return text\n",
        "\n",
        "# Test normalization function\n",
        "print(\"Testing text normalization:\\n\")\n",
        "test_cases = [\n",
        "    \"José María O'Brien\",\n",
        "    \"AL-QAIDA\",\n",
        "    \"Société Générale\",\n",
        "    \"中国工商银行\",  # Chinese - will be stripped (OFAC uses romanized names)\n",
        "    \"  Multiple   Spaces  \",\n",
        "    \"UPPER-case-MiXeD\",\n",
        "]\n",
        "\n",
        "for test in test_cases:\n",
        "    normalized = normalize_text(test)\n",
        "    # Show empty string explicitly for clarity\n",
        "    display_normalized = f\"'{normalized}'\" if normalized else \"''\" \n",
        "    print(f\"  '{test}' → {display_normalized}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load OFAC Data Files\n",
        "\n",
        "We load all OFAC sanctions lists with explicit column mappings since OFAC CSV files don't include headers. We're loading six files total:\n",
        "- **SDN List**: Primary names, alternate names, addresses\n",
        "- **Consolidated List**: Primary names, alternate names, addresses\n",
        "\n",
        "Each sanctions entry can have multiple alternate names (aliases, former names, etc.) and multiple addresses with country information. We'll merge these together to create a comprehensive screening database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading OFAC Sanctions Lists...\n",
            "\n",
            "Loading SDN List...\n",
            " - Primary entities: 17,945\n",
            " - Alternate names:  19,898\n",
            " - Addresses:        23,628\n",
            "\n",
            "Loading Consolidated List...\n",
            " - Primary entities: 444\n",
            " - Alternate names:  1,067\n",
            " - Addresses:        573\n",
            "\n",
            "All OFAC files loaded successfully\n"
          ]
        }
      ],
      "source": [
        "# Define column mappings for OFAC CSV files (they have no headers)\n",
        "PRIMARY_COLS = [\n",
        "    'ent_num', 'SDN_Name', 'SDN_Type', 'Program', 'Title',\n",
        "    'Call_Sign', 'Vess_type', 'Tonnage', 'GRT', 'Vess_flag',\n",
        "    'Vess_owner', 'Remarks'\n",
        "]\n",
        "\n",
        "ALT_COLS = ['ent_num', 'alt_num', 'alt_type', 'alt_name', 'alt_remarks']\n",
        "\n",
        "ADD_COLS = [\n",
        "    'ent_num', 'Add_num', 'Address', 'City_State_Province',\n",
        "    'Country', 'Add_Remarks'\n",
        "]\n",
        "\n",
        "print(\"Loading OFAC Sanctions Lists...\\n\")\n",
        "\n",
        "# Load SDN (Specially Designated Nationals) List\n",
        "print(\"Loading SDN List...\")\n",
        "sdn_primary = pd.read_csv(\n",
        "    OFAC_DIR / 'sdn' / 'sdn.csv',\n",
        "    header=None,\n",
        "    names=PRIMARY_COLS,\n",
        "    dtype={'ent_num': str},\n",
        "    encoding='utf-8'\n",
        ")\n",
        "\n",
        "sdn_alt = pd.read_csv(\n",
        "    OFAC_DIR / 'sdn' / 'alt.csv',\n",
        "    header=None,\n",
        "    names=ALT_COLS,\n",
        "    dtype={'ent_num': str, 'alt_num': str},\n",
        "    encoding='utf-8'\n",
        ")\n",
        "\n",
        "sdn_add = pd.read_csv(\n",
        "    OFAC_DIR / 'sdn' / 'add.csv',\n",
        "    header=None,\n",
        "    names=ADD_COLS,\n",
        "    dtype={'ent_num': str, 'Add_num': str},\n",
        "    encoding='utf-8'\n",
        ")\n",
        "\n",
        "print(f\" - Primary entities: {len(sdn_primary):,}\")\n",
        "print(f\" - Alternate names:  {len(sdn_alt):,}\")\n",
        "print(f\" - Addresses:        {len(sdn_add):,}\")\n",
        "\n",
        "# Load Consolidated List\n",
        "print(\"\\nLoading Consolidated List...\")\n",
        "cons_primary = pd.read_csv(\n",
        "    OFAC_DIR / 'consolidated' / 'cons_prim.csv',\n",
        "    header=None,\n",
        "    names=PRIMARY_COLS,\n",
        "    dtype={'ent_num': str},\n",
        "    encoding='utf-8'\n",
        ")\n",
        "\n",
        "cons_alt = pd.read_csv(\n",
        "    OFAC_DIR / 'consolidated' / 'cons_alt.csv',\n",
        "    header=None,\n",
        "    names=ALT_COLS,\n",
        "    dtype={'ent_num': str, 'alt_num': str},\n",
        "    encoding='utf-8'\n",
        ")\n",
        "\n",
        "cons_add = pd.read_csv(\n",
        "    OFAC_DIR / 'consolidated' / 'cons_add.csv',\n",
        "    header=None,\n",
        "    names=ADD_COLS,\n",
        "    dtype={'ent_num': str, 'Add_num': str},\n",
        "    encoding='utf-8'\n",
        ")\n",
        "\n",
        "print(f\" - Primary entities: {len(cons_primary):,}\")\n",
        "print(f\" - Alternate names:  {len(cons_alt):,}\")\n",
        "print(f\" - Addresses:        {len(cons_add):,}\")\n",
        "\n",
        "print(\"\\nAll OFAC files loaded successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
