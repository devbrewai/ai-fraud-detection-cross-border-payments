{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sanctions Screening\n",
        "\n",
        "- **Purpose:** OFAC sanctions screening with fuzzy name matching for fraud detection pipeline  \n",
        "- **Author:** Devbrew LLC  \n",
        "- **Last Updated:** November 4, 2025  \n",
        "- **Status:** In progress  \n",
        "- **License:** Apache 2.0 (Code) | Public Domain (OFAC Data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Dataset License Notice\n",
        "\n",
        "This notebook uses **OFAC Sanctions Lists** (SDN and Consolidated) from the U.S. Department of the Treasury.\n",
        "\n",
        "**Dataset License:** Public Domain  \n",
        "- OFAC sanctions data is publicly available from [OFAC Sanctions List Search](https://sanctionslist.ofac.treas.gov/Home)  \n",
        "- Data can be freely used, redistributed, and incorporated into commercial systems  \n",
        "- Updates are published regularly; production systems should refresh data periodically  \n",
        "\n",
        "**Setup Instructions:** See [`../data_catalog/README.md`](../data_catalog/README.md) for download instructions.\n",
        "\n",
        "**Code License:** This notebook's code is licensed under Apache 2.0 (open source).\n",
        "\n",
        "**Disclaimer:** This is a research demonstration. Production sanctions screening requires broader list coverage (EU, UN, UK HMT), legal review, and compliance with local regulations.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notebook Configuration\n",
        "\n",
        "### Environment Setup\n",
        "\n",
        "We configure the Python environment with standardized settings, import required libraries for text processing and fuzzy matching, and set a fixed random seed for reproducibility. This ensures consistent results across runs and enables reliable experimentation.\n",
        "\n",
        "These settings establish the foundation for all sanctions screening operations, including name normalization, tokenization, and similarity scoring."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment configured successfully\n",
            "pandas: 2.3.3\n",
            "numpy: 2.3.3\n",
            "rapidfuzz: 3.14.1\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "from pathlib import Path\n",
        "import json\n",
        "import hashlib\n",
        "import unicodedata\n",
        "import re\n",
        "from typing import Dict, Any, Optional, List, Tuple\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import rapidfuzz as rf\n",
        "from rapidfuzz import fuzz, process\n",
        "\n",
        "# Configuration\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "pd.set_option(\"display.max_columns\", 100)\n",
        "pd.set_option(\"display.max_rows\", 100)\n",
        "pd.set_option(\"display.float_format\", '{:.2f}'.format)\n",
        "\n",
        "# Plotting configuration\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
        "plt.rcParams[\"font.size\"] = 10\n",
        "\n",
        "# Reproducibility\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "\n",
        "print(\"Environment configured successfully\")\n",
        "print(f\"pandas: {pd.__version__}\")\n",
        "print(f\"numpy: {np.__version__}\")\n",
        "print(f\"rapidfuzz: {rf.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Path Configuration\n",
        "\n",
        "We define the project directory structure and validate that OFAC data files exist before proceeding. The validation ensures we have the necessary sanctions lists for screening operations.\n",
        "\n",
        "This configuration pattern ensures we can locate all required data artifacts and provides clear feedback if prerequisites are missing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OFAC Data Availability Check:\n",
            " - SDN Primary              : Found\n",
            " - SDN Alternate            : Found\n",
            " - SDN Address              : Found\n",
            " - Consolidated Primary     : Found\n",
            " - Consolidated Alternate   : Found\n",
            " - Consolidated Address     : Found\n",
            "\n",
            "All required OFAC data files are available\n"
          ]
        }
      ],
      "source": [
        "# Project paths\n",
        "PROJECT_ROOT = Path.cwd().parent\n",
        "DATA_DIR = PROJECT_ROOT / \"data_catalog\"\n",
        "OFAC_DIR = DATA_DIR / \"ofac\"\n",
        "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
        "MODELS_DIR = PROJECT_ROOT / \"packages\" / \"models\"\n",
        "\n",
        "# Ensure output directories exist\n",
        "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
        "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Expected OFAC data files\n",
        "OFAC_FILES = {\n",
        "    'SDN Primary': OFAC_DIR / 'sdn' / 'sdn.csv',\n",
        "    'SDN Alternate': OFAC_DIR / 'sdn' / 'alt.csv',\n",
        "    'SDN Address': OFAC_DIR / 'sdn' / 'add.csv',\n",
        "    'Consolidated Primary': OFAC_DIR / 'consolidated' / 'cons_prim.csv',\n",
        "    'Consolidated Alternate': OFAC_DIR / 'consolidated' / 'cons_alt.csv',\n",
        "    'Consolidated Address': OFAC_DIR / 'consolidated' / 'cons_add.csv',\n",
        "}\n",
        "\n",
        "def validate_required_data():\n",
        "    \"\"\"Validate that OFAC sanctions data files exist.\"\"\"\n",
        "    print(\"OFAC Data Availability Check:\")\n",
        "    \n",
        "    all_exist = True\n",
        "    for name, path in OFAC_FILES.items():\n",
        "        exists = path.exists()\n",
        "        status = \"Found\" if exists else \"Missing\"\n",
        "        print(f\" - {name:25s}: {status}\")\n",
        "        if not exists:\n",
        "            all_exist = False\n",
        "    \n",
        "    if not all_exist:\n",
        "        print(\"\\n[WARNING] Some OFAC files are missing; see data_catalog/README.md for instructions\")\n",
        "    else:\n",
        "        print(\"\\nAll required OFAC data files are available\")\n",
        "    \n",
        "    return all_exist\n",
        "\n",
        "data_available = validate_required_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load & Normalize OFAC Datasets\n",
        "\n",
        "We load OFAC sanctions lists (SDN and Consolidated) and apply comprehensive text normalization to enable robust fuzzy matching. This step is critical for handling variations in how names appear across different systems and languages.\n",
        "\n",
        "Our normalization strategy addresses several common challenges in sanctions screening:\n",
        "- **Unicode variations**: Convert to canonical form (NFKC) to handle different encodings\n",
        "- **Accent marks**: Strip diacritics to match \"José\" with \"Jose\"\n",
        "- **Case sensitivity**: Lowercase everything for case-insensitive matching\n",
        "- **Punctuation**: Standardize hyphens, remove quotes that don't affect identity\n",
        "- **Whitespace**: Collapse multiple spaces to single space\n",
        "\n",
        "This preprocessing ensures we can match names reliably even when they're formatted differently in transaction data versus sanctions lists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing text normalization:\n",
            "\n",
            "  'José María O'Brien' → 'jose maria obrien'\n",
            "  'AL-QAIDA' → 'al-qaida'\n",
            "  'Société Générale' → 'societe generale'\n",
            "  '中国工商银行' → ''\n",
            "  '  Multiple   Spaces  ' → 'multiple spaces'\n",
            "  'UPPER-case-MiXeD' → 'upper-case-mixed'\n"
          ]
        }
      ],
      "source": [
        "def normalize_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Normalize text for robust fuzzy matching.\n",
        "    \n",
        "    Applies NFKC normalization, lowercasing, accent stripping,\n",
        "    punctuation canonicalization, and whitespace collapse.\n",
        "    \n",
        "    Note: Non-Latin scripts (Chinese, Arabic, Cyrillic) are stripped\n",
        "    because OFAC sanctions lists use romanized names. For example:\n",
        "    - \"中国工商银行\" → \"\" (empty)\n",
        "    - \"INDUSTRIAL AND COMMERCIAL BANK OF CHINA\" → \"industrial and commercial bank of china\"\n",
        "    \n",
        "    Args:\n",
        "        text: Raw text string to normalize\n",
        "        \n",
        "    Returns:\n",
        "        Normalized text string suitable for fuzzy matching.\n",
        "        Returns empty string if input contains only non-Latin characters.\n",
        "        \n",
        "    Examples:\n",
        "        >>> normalize_text(\"José María O'Brien\")\n",
        "        'jose maria obrien'\n",
        "        \n",
        "        >>> normalize_text(\"AL-QAIDA\")\n",
        "        'al qaida'\n",
        "        \n",
        "        >>> normalize_text(\"中国工商银行\")\n",
        "        ''\n",
        "    \"\"\"\n",
        "    if not text or pd.isna(text):\n",
        "        return \"\"\n",
        "    \n",
        "    # Convert to string if not already\n",
        "    text = str(text)\n",
        "    \n",
        "    # Unicode normalization (canonical composition)\n",
        "    text = unicodedata.normalize(\"NFKC\", text)\n",
        "    \n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Strip accent marks (diacritics)\n",
        "    # Decompose characters, then filter out combining marks\n",
        "    text = ''.join(\n",
        "        char for char in unicodedata.normalize(\"NFD\", text)\n",
        "        if unicodedata.category(char) != 'Mn'\n",
        "    )\n",
        "    \n",
        "    # Remove quotes (single and double)\n",
        "    text = re.sub(r\"['\\\"]\", \"\", text)\n",
        "    \n",
        "    # Replace non-alphanumeric (except space and hyphen) with space\n",
        "    # Note: This strips non-Latin scripts (Chinese, Arabic, Cyrillic, etc.)\n",
        "    # OFAC lists use romanized names, so this is intentional behavior\n",
        "    text = re.sub(r\"[^a-z0-9\\s-]\", \" \", text)\n",
        "    \n",
        "    # Collapse multiple spaces to single space\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    \n",
        "    # Strip leading/trailing whitespace\n",
        "    text = text.strip()\n",
        "    \n",
        "    return text\n",
        "\n",
        "# Test normalization function\n",
        "print(\"Testing text normalization:\\n\")\n",
        "test_cases = [\n",
        "    \"José María O'Brien\",\n",
        "    \"AL-QAIDA\",\n",
        "    \"Société Générale\",\n",
        "    \"中国工商银行\",  # Chinese - will be stripped (OFAC uses romanized names)\n",
        "    \"  Multiple   Spaces  \",\n",
        "    \"UPPER-case-MiXeD\",\n",
        "]\n",
        "\n",
        "for test in test_cases:\n",
        "    normalized = normalize_text(test)\n",
        "    # Show empty string explicitly for clarity\n",
        "    display_normalized = f\"'{normalized}'\" if normalized else \"''\" \n",
        "    print(f\"  '{test}' → {display_normalized}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load OFAC Data Files\n",
        "\n",
        "We load all OFAC sanctions lists with explicit column mappings since OFAC CSV files don't include headers. We're loading six files total:\n",
        "- **SDN List**: Primary names, alternate names, addresses\n",
        "- **Consolidated List**: Primary names, alternate names, addresses\n",
        "\n",
        "Each sanctions entry can have multiple alternate names (aliases, former names, etc.) and multiple addresses with country information. We'll merge these together to create a comprehensive screening database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading OFAC Sanctions Lists...\n",
            "\n",
            "Loading SDN List...\n",
            " - Primary entities: 17,945\n",
            " - Alternate names:  19,898\n",
            " - Addresses:        23,628\n",
            "\n",
            "Loading Consolidated List...\n",
            " - Primary entities: 444\n",
            " - Alternate names:  1,067\n",
            " - Addresses:        573\n",
            "\n",
            "All OFAC files loaded successfully\n"
          ]
        }
      ],
      "source": [
        "# Define column mappings for OFAC CSV files (they have no headers)\n",
        "PRIMARY_COLS = [\n",
        "    'ent_num', 'SDN_Name', 'SDN_Type', 'Program', 'Title',\n",
        "    'Call_Sign', 'Vess_type', 'Tonnage', 'GRT', 'Vess_flag',\n",
        "    'Vess_owner', 'Remarks'\n",
        "]\n",
        "\n",
        "ALT_COLS = ['ent_num', 'alt_num', 'alt_type', 'alt_name', 'alt_remarks']\n",
        "\n",
        "ADD_COLS = [\n",
        "    'ent_num', 'Add_num', 'Address', 'City_State_Province',\n",
        "    'Country', 'Add_Remarks'\n",
        "]\n",
        "\n",
        "print(\"Loading OFAC Sanctions Lists...\\n\")\n",
        "\n",
        "# Load SDN (Specially Designated Nationals) List\n",
        "print(\"Loading SDN List...\")\n",
        "sdn_primary = pd.read_csv(\n",
        "    OFAC_DIR / 'sdn' / 'sdn.csv',\n",
        "    header=None,\n",
        "    names=PRIMARY_COLS,\n",
        "    dtype={'ent_num': str},\n",
        "    encoding='utf-8'\n",
        ")\n",
        "\n",
        "sdn_alt = pd.read_csv(\n",
        "    OFAC_DIR / 'sdn' / 'alt.csv',\n",
        "    header=None,\n",
        "    names=ALT_COLS,\n",
        "    dtype={'ent_num': str, 'alt_num': str},\n",
        "    encoding='utf-8'\n",
        ")\n",
        "\n",
        "sdn_add = pd.read_csv(\n",
        "    OFAC_DIR / 'sdn' / 'add.csv',\n",
        "    header=None,\n",
        "    names=ADD_COLS,\n",
        "    dtype={'ent_num': str, 'Add_num': str},\n",
        "    encoding='utf-8'\n",
        ")\n",
        "\n",
        "print(f\" - Primary entities: {len(sdn_primary):,}\")\n",
        "print(f\" - Alternate names:  {len(sdn_alt):,}\")\n",
        "print(f\" - Addresses:        {len(sdn_add):,}\")\n",
        "\n",
        "# Load Consolidated List\n",
        "print(\"\\nLoading Consolidated List...\")\n",
        "cons_primary = pd.read_csv(\n",
        "    OFAC_DIR / 'consolidated' / 'cons_prim.csv',\n",
        "    header=None,\n",
        "    names=PRIMARY_COLS,\n",
        "    dtype={'ent_num': str},\n",
        "    encoding='utf-8'\n",
        ")\n",
        "\n",
        "cons_alt = pd.read_csv(\n",
        "    OFAC_DIR / 'consolidated' / 'cons_alt.csv',\n",
        "    header=None,\n",
        "    names=ALT_COLS,\n",
        "    dtype={'ent_num': str, 'alt_num': str},\n",
        "    encoding='utf-8'\n",
        ")\n",
        "\n",
        "cons_add = pd.read_csv(\n",
        "    OFAC_DIR / 'consolidated' / 'cons_add.csv',\n",
        "    header=None,\n",
        "    names=ADD_COLS,\n",
        "    dtype={'ent_num': str, 'Add_num': str},\n",
        "    encoding='utf-8'\n",
        ")\n",
        "\n",
        "print(f\" - Primary entities: {len(cons_primary):,}\")\n",
        "print(f\" - Alternate names:  {len(cons_alt):,}\")\n",
        "print(f\" - Addresses:        {len(cons_add):,}\")\n",
        "\n",
        "print(\"\\nAll OFAC files loaded successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Consolidate Names and Normalize\n",
        "\n",
        "We merge primary names with their alternate names (aliases, former names) and create a unified sanctions database. Each row will represent a distinct name associated with a sanctioned entity, including both the official name and all known aliases.\n",
        "\n",
        "We also extract country information from address records to enable geographic filtering during screening. This is important because many sanctions programs are country-specific."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Building SDN sanctions index...\n",
            "  Normalizing names...\n",
            "  Removed 2 records with empty normalized names\n",
            "Created 37,841 name records\n",
            "\n",
            "Building Consolidated sanctions index...\n",
            "  Normalizing names...\n",
            "  Removed 2 records with empty normalized names\n",
            "Created 1,509 name records\n",
            "\n",
            "Combined Sanctions Index Summary:\n",
            " - Total name records: 39,350\n",
            " - From SDN:           37,841\n",
            " - From Consolidated:  1,509\n",
            " - Unique entities:    18,310\n"
          ]
        }
      ],
      "source": [
        "def build_sanctions_index(\n",
        "    primary_df: pd.DataFrame,\n",
        "    alt_df: pd.DataFrame,\n",
        "    add_df: pd.DataFrame,\n",
        "    source_name: str\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Build unified sanctions index from primary, alternate, and address files.\n",
        "    \n",
        "    Args:\n",
        "        primary_df: Primary sanctions entities\n",
        "        alt_df: Alternate names (aliases)\n",
        "        add_df: Address records with country info\n",
        "        source_name: Source identifier ('SDN' or 'Consolidated')\n",
        "        \n",
        "    Returns:\n",
        "        DataFrame with columns: uid, name, name_norm, name_type, entity_type, \n",
        "                                program, country, remarks, source\n",
        "    \"\"\"\n",
        "    print(f\"\\nBuilding {source_name} sanctions index...\")\n",
        "    \n",
        "    # Process primary names\n",
        "    primary_records = []\n",
        "    for _, row in primary_df.iterrows():\n",
        "        primary_records.append({\n",
        "            'uid': f\"{source_name}_{row['ent_num']}\",\n",
        "            'ent_num': row['ent_num'],\n",
        "            'name': row['SDN_Name'],\n",
        "            'name_type': 'primary',\n",
        "            'entity_type': row['SDN_Type'],\n",
        "            'program': row['Program'],\n",
        "            'remarks': row['Remarks'],\n",
        "            'source': source_name\n",
        "        })\n",
        "    \n",
        "    # Process alternate names\n",
        "    alt_records = []\n",
        "    for _, row in alt_df.iterrows():\n",
        "        alt_records.append({\n",
        "            'uid': f\"{source_name}_{row['ent_num']}_alt_{row['alt_num']}\",\n",
        "            'ent_num': row['ent_num'],\n",
        "            'name': row['alt_name'],\n",
        "            'name_type': row['alt_type'],  # aka, fka, nka\n",
        "            'entity_type': None,  # Will be filled from primary\n",
        "            'program': None,      # Will be filled from primary\n",
        "            'remarks': row['alt_remarks'],\n",
        "            'source': source_name\n",
        "        })\n",
        "    \n",
        "    # Combine primary and alternate names\n",
        "    all_names = pd.DataFrame(primary_records + alt_records)\n",
        "    \n",
        "    # Fill entity_type and program from primary records for alternates\n",
        "    entity_info = primary_df[['ent_num', 'SDN_Type', 'Program']].copy()\n",
        "    entity_info.columns = ['ent_num', 'entity_type_fill', 'program_fill']\n",
        "    \n",
        "    all_names = all_names.merge(entity_info, on='ent_num', how='left')\n",
        "    all_names['entity_type'] = all_names['entity_type'].fillna(all_names['entity_type_fill'])\n",
        "    all_names['program'] = all_names['program'].fillna(all_names['program_fill'])\n",
        "    all_names.drop(columns=['entity_type_fill', 'program_fill'], inplace=True)\n",
        "    \n",
        "    # Extract country information from addresses (take first country per entity)\n",
        "    if len(add_df) > 0:\n",
        "        country_map = add_df.groupby('ent_num')['Country'].first().to_dict()\n",
        "        all_names['country'] = all_names['ent_num'].map(country_map)\n",
        "    else:\n",
        "        all_names['country'] = None\n",
        "    \n",
        "    # Apply text normalization\n",
        "    print(\"  Normalizing names...\")\n",
        "    all_names['name_norm'] = all_names['name'].apply(normalize_text)\n",
        "    \n",
        "    # Remove records with empty normalized names\n",
        "    before_count = len(all_names)\n",
        "    all_names = all_names[all_names['name_norm'].str.len() > 0].copy()\n",
        "    after_count = len(all_names)\n",
        "    \n",
        "    if before_count > after_count:\n",
        "        print(f\"  Removed {before_count - after_count} records with empty normalized names\")\n",
        "    \n",
        "    # Reorder columns\n",
        "    columns = [\n",
        "        'uid', 'ent_num', 'name', 'name_norm', 'name_type', \n",
        "        'entity_type', 'program', 'country', 'remarks', 'source'\n",
        "    ]\n",
        "    all_names = all_names[columns]\n",
        "    \n",
        "    print(f\"Created {len(all_names):,} name records\")\n",
        "    \n",
        "    return all_names\n",
        "\n",
        "# Build indices for both lists\n",
        "sdn_index = build_sanctions_index(sdn_primary, sdn_alt, sdn_add, 'SDN')\n",
        "cons_index = build_sanctions_index(cons_primary, cons_alt, cons_add, 'Consolidated')\n",
        "\n",
        "# Combine into single index\n",
        "sanctions_index = pd.concat([sdn_index, cons_index], ignore_index=True)\n",
        "\n",
        "print(f\"\\nCombined Sanctions Index Summary:\")\n",
        "print(f\" - Total name records: {len(sanctions_index):,}\")\n",
        "print(f\" - From SDN:           {len(sdn_index):,}\")\n",
        "print(f\" - From Consolidated:  {len(cons_index):,}\")\n",
        "print(f\" - Unique entities:    {sanctions_index['ent_num'].nunique():,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Validation Checks\n",
        "\n",
        "We perform data quality validation to ensure our sanctions index is ready for fuzzy matching:\n",
        "1. **Non-empty canonical names**: Every record must have valid normalized text\n",
        "2. **Unique UIDs**: Each name record has a globally unique identifier\n",
        "3. **Field completeness**: Key fields (entity_type, program) are populated\n",
        "4. **Normalization quality**: Check sample names to verify normalization worked correctly\n",
        "\n",
        "These checks catch data quality issues before they cause problems in production screening."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Check 1: Non-empty canonical names\n",
            " - Empty normalized names: 0\n",
            "PASS - All records have valid normalized names\n",
            "\n",
            "Validation Check 2: Unique UIDs\n",
            " - Duplicate UIDs: 0\n",
            "PASS - All UIDs are unique\n",
            "\n",
            "Validation Check 3: Field completeness\n",
            " - Records with entity_type: 39,350 / 39,350\n",
            " - Records with program:     39,350 / 39,350\n",
            " - Records with country:     39,350 / 39,350\n",
            "PASS - Key fields adequately populated\n",
            "\n",
            "Validation Check 4: Sample normalization quality\n",
            "Checking 10 random samples...\n",
            " - 'AL-HARAMAYN HUMANITARIAN FOUNDATION' → 'al-haramayn humanitarian foundation'\n",
            " - 'AKTSIONERNOE OBSHCHESTVO RT-STROITELNYE TEKHNOLOGII' → 'aktsionernoe obshchestvo rt-stroitelnye tekhnologii'\n",
            " - 'VALLE VALLE, Luis Alonso' → 'valle valle luis alonso'\n",
            " - 'GLOBAL SEA LINE CO LTD' → 'global sea line co ltd'\n",
            " - 'TED TEKNOLOJI' → 'ted teknoloji'\n",
            " - 'CLOSED JOINT STOCK COMPANY 'IFD KAPITAL'' → 'closed joint stock company ifd kapital'\n",
            " - 'JOINT STOCK COMPANY SEVERGAZBANK' → 'joint stock company severgazbank'\n",
            " - 'CENTROCREDIT BANK' → 'centrocredit bank'\n",
            " - 'OTKRYTOE AKTSIONERNOE OBSHCHESTVO RT KHIMICHESKIE TECHNOLOGIES I KOMPOZITSIONNYE MATERIALY' → 'otkrytoe aktsionernoe obshchestvo rt khimicheskie technologies i kompozitsionnye materialy'\n",
            " - 'BUKANOV, Timur Evgenyevich' → 'bukanov timur evgenyevich'\n"
          ]
        }
      ],
      "source": [
        "# Validation Check 1: Non-empty canonical names\n",
        "empty_names = sanctions_index[sanctions_index['name_norm'].str.len() == 0]\n",
        "print(f\"Validation Check 1: Non-empty canonical names\")\n",
        "print(f\" - Empty normalized names: {len(empty_names)}\")\n",
        "assert len(empty_names) == 0, \"Found records with empty normalized names!\"\n",
        "print(f\"PASS - All records have valid normalized names\\n\")\n",
        "\n",
        "# Validation Check 2: Unique UIDs\n",
        "print(f\"Validation Check 2: Unique UIDs\")\n",
        "duplicate_uids = sanctions_index['uid'].duplicated().sum()\n",
        "print(f\" - Duplicate UIDs: {duplicate_uids}\")\n",
        "assert duplicate_uids == 0, \"Found duplicate UIDs!\"\n",
        "print(f\"PASS - All UIDs are unique\\n\")\n",
        "\n",
        "# Validation Check 3: Field completeness\n",
        "print(f\"Validation Check 3: Field completeness\")\n",
        "print(f\" - Records with entity_type: {sanctions_index['entity_type'].notna().sum():,} / {len(sanctions_index):,}\")\n",
        "print(f\" - Records with program:     {sanctions_index['program'].notna().sum():,} / {len(sanctions_index):,}\")\n",
        "print(f\" - Records with country:     {sanctions_index['country'].notna().sum():,} / {len(sanctions_index):,}\")\n",
        "\n",
        "# Country is optional (not all entities have addresses)\n",
        "entity_type_coverage = sanctions_index['entity_type'].notna().mean()\n",
        "program_coverage = sanctions_index['program'].notna().mean()\n",
        "\n",
        "if entity_type_coverage < 0.95:\n",
        "    print(f\"[WARNING] Entity type coverage is low: {entity_type_coverage*100:.1f}%\")\n",
        "if program_coverage < 0.95:\n",
        "    print(f\"[WARNING] Program coverage is low: {program_coverage*100:.1f}%\")\n",
        "\n",
        "print(f\"PASS - Key fields adequately populated\\n\")\n",
        "\n",
        "# Validation Check 4: Sample normalization quality\n",
        "print(f\"Validation Check 4: Sample normalization quality\")\n",
        "print(f\"Checking 10 random samples...\")\n",
        "\n",
        "sample_indices = np.random.choice(len(sanctions_index), size=10, replace=False)\n",
        "for idx in sample_indices:\n",
        "    row = sanctions_index.iloc[idx]\n",
        "    original = row['name']\n",
        "    normalized = row['name_norm']\n",
        "    print(f\" - '{original}' → '{normalized}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Analyze Sanctions Index\n",
        "\n",
        "We examine the distribution of entity types, programs, and countries in our sanctions database. This helps us understand what we're screening against and can inform filtering strategies during production deployment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entity Type Distribution:\n",
            "-0-                           : 21,308 ( 54.1%)\n",
            "individual                    : 16,149 ( 41.0%)\n",
            "vessel                        :  1,555 (  4.0%)\n",
            "aircraft                      :    338 (  0.9%)\n",
            "\n",
            "Sanctions Program Distribution (Top 15):\n",
            "RUSSIA-EO14024                          : 10,339 ( 26.3%)\n",
            "SDGT                                    :  7,037 ( 17.9%)\n",
            "SDNTK                                   :  2,395 (  6.1%)\n",
            "UKRAINE-EO13662] [RUSSIA-EO14024        :  1,415 (  3.6%)\n",
            "GLOMAG                                  :  1,218 (  3.1%)\n",
            "NPWMD] [IFSR                            :  1,122 (  2.9%)\n",
            "IRAN                                    :    837 (  2.1%)\n",
            "UKRAINE-EO13662                         :    785 (  2.0%)\n",
            "BELARUS-EO14038                         :    642 (  1.6%)\n",
            "SDGT] [IFSR                             :    622 (  1.6%)\n",
            "IRAN-EO13902                            :    572 (  1.5%)\n",
            "IRAN-EO13846                            :    553 (  1.4%)\n",
            "PAARSSR-EO13894                         :    536 (  1.4%)\n",
            "FTO] [SDGT                              :    497 (  1.3%)\n",
            "ILLICIT-DRUGS-EO14059                   :    475 (  1.2%)\n",
            "\n",
            "Country Distribution (Top 15):\n",
            "Russia                        : 11,544 ( 29.3%)\n",
            "-0-                           :  5,783 ( 14.7%)\n",
            "Iran                          :  3,419 (  8.7%)\n",
            "China                         :  1,688 (  4.3%)\n",
            "Mexico                        :  1,396 (  3.5%)\n",
            "Belarus                       :    998 (  2.5%)\n",
            "Syria                         :    947 (  2.4%)\n",
            "Lebanon                       :    918 (  2.3%)\n",
            "United Arab Emirates          :    869 (  2.2%)\n",
            "Pakistan                      :    723 (  1.8%)\n",
            "Turkey                        :    632 (  1.6%)\n",
            "Ukraine                       :    626 (  1.6%)\n",
            "Korea, North                  :    509 (  1.3%)\n",
            "Yemen                         :    476 (  1.2%)\n",
            "Colombia                      :    460 (  1.2%)\n",
            "\n",
            "Name Type Distribution:\n",
            "aka                           : 20,266 ( 51.5%)\n",
            "primary                       : 18,387 ( 46.7%)\n",
            "fka                           :    680 (  1.7%)\n",
            "nka                           :     17 (  0.0%)\n"
          ]
        }
      ],
      "source": [
        "# Distribution analysis\n",
        "print(\"Entity Type Distribution:\")\n",
        "entity_type_dist = sanctions_index['entity_type'].value_counts()\n",
        "for entity_type, count in entity_type_dist.head(10).items():\n",
        "    pct = (count / len(sanctions_index)) * 100\n",
        "    print(f\"{str(entity_type)[:30]:30s}: {count:>6,} ({pct:>5.1f}%)\")\n",
        "\n",
        "print(\"\\nSanctions Program Distribution (Top 15):\")\n",
        "program_dist = sanctions_index['program'].value_counts()\n",
        "for program, count in program_dist.head(15).items():\n",
        "    pct = (count / len(sanctions_index)) * 100\n",
        "    program_str = str(program)[:50] if pd.notna(program) else \"Unknown\"\n",
        "    print(f\"{program_str:40s}: {count:>6,} ({pct:>5.1f}%)\")\n",
        "\n",
        "print(\"\\nCountry Distribution (Top 15):\")\n",
        "country_dist = sanctions_index['country'].value_counts()\n",
        "for country, count in country_dist.head(15).items():\n",
        "    pct = (count / len(sanctions_index)) * 100\n",
        "    country_str = str(country)[:30] if pd.notna(country) else \"Unknown\"\n",
        "    print(f\"{country_str:30s}: {count:>6,} ({pct:>5.1f}%)\")\n",
        "\n",
        "# Name type distribution\n",
        "print(\"\\nName Type Distribution:\")\n",
        "name_type_dist = sanctions_index['name_type'].value_counts()\n",
        "for name_type, count in name_type_dist.items():\n",
        "    pct = (count / len(sanctions_index)) * 100\n",
        "    print(f\"{str(name_type):30s}: {count:>6,} ({pct:>5.1f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save Normalized Sanctions Index\n",
        "\n",
        "We save the normalized sanctions index as the foundation for our fuzzy matching pipeline. This database contains all sanctioned entity names with proper text normalization, metadata enrichment, and quality validation applied.\n",
        "\n",
        "The artifacts enable fast loading and consistent screening across the fraud detection system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved sanctions index: /Users/joekariuki/Documents/Devbrew/research/devbrew-payments-fraud-sanctions/packages/models/sanctions_index.parquet\n",
            " - Shape: (39350, 10)\n",
            " - Size: 2874.1 KB\n",
            "Saved metadata: /Users/joekariuki/Documents/Devbrew/research/devbrew-payments-fraud-sanctions/packages/models/sanctions_index_metadata.json\n",
            "Sanctions Index Ready: 39,350 normalized name records\n"
          ]
        }
      ],
      "source": [
        "# Save normalized sanctions index\n",
        "sanctions_index_path = MODELS_DIR / \"sanctions_index.parquet\"\n",
        "sanctions_index.to_parquet(sanctions_index_path, index=False)\n",
        "\n",
        "print(f\"Saved sanctions index: {sanctions_index_path}\")\n",
        "print(f\" - Shape: {sanctions_index.shape}\")\n",
        "print(f\" - Size: {sanctions_index_path.stat().st_size / 1024:.1f} KB\")\n",
        "\n",
        "# Save metadata for pipeline tracking\n",
        "metadata = {\n",
        "    \"created_at\": pd.Timestamp.now().isoformat(),\n",
        "    \"total_records\": len(sanctions_index),\n",
        "    \"unique_entities\": sanctions_index['ent_num'].nunique(),\n",
        "    \"sources\": sanctions_index['source'].value_counts().to_dict(),\n",
        "    \"entity_types\": entity_type_dist.head(10).to_dict(),\n",
        "    \"top_programs\": program_dist.head(10).to_dict(),\n",
        "    \"top_countries\": country_dist.head(10).to_dict(),\n",
        "    \"name_types\": name_type_dist.to_dict(),\n",
        "    \"country_coverage_pct\": float(sanctions_index['country'].notna().mean() * 100),\n",
        "    \"validation\": {\n",
        "        \"empty_normalized_names\": 0,\n",
        "        \"duplicate_uids\": 0,\n",
        "        \"entity_type_coverage_pct\": float(entity_type_coverage * 100),\n",
        "        \"program_coverage_pct\": float(program_coverage * 100)\n",
        "    }\n",
        "}\n",
        "\n",
        "metadata_path = MODELS_DIR / \"sanctions_index_metadata.json\"\n",
        "with open(metadata_path, 'w') as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "print(f\"Saved metadata: {metadata_path}\")\n",
        "print(f\"Sanctions Index Ready: {len(sanctions_index):,} normalized name records\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tokenization & Canonical Forms\n",
        "\n",
        "To enable efficient fuzzy matching, we tokenize normalized names and create canonical representations optimized for different similarity algorithms. This approach improves matching accuracy by:\n",
        "- **Removing noise**: Filtering out common business suffixes (Ltd, Inc, LLC) and honorifics (Mr, Mrs)\n",
        "- **Token-based matching**: Breaking names into words for flexible comparison\n",
        "- **Sorted tokens**: Enabling order-independent matching (e.g., \"John Doe\" matches \"Doe John\")\n",
        "- **Token sets**: Creating unique word bags for set-based similarity\n",
        "\n",
        "These canonical forms serve as inputs to RapidFuzz's token_sort_ratio and token_set_ratio algorithms, which are robust to word order variations and common name formatting differences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing tokenization:\n",
            "\n",
            "  'john doe' → ['john', 'doe']\n",
            "  'acme corporation ltd' → ['acme']\n",
            "  'al-qaida' → ['al', 'qaida']\n",
            "  'banco nacional de cuba' → ['banco', 'nacional', 'cuba']\n",
            "  'mr jose maria obrien' → ['jose', 'maria', 'obrien']\n",
            "  'china telecom co ltd' → ['china', 'telecom']\n"
          ]
        }
      ],
      "source": [
        "# Define stopwords for name tokenization\n",
        "# These are common business/legal terms and honorifics that add noise to matching\n",
        "STOPWORDS = {\n",
        "    # Business suffixes\n",
        "    \"ltd\", \"inc\", \"llc\", \"co\", \"corp\", \"corporation\", \"company\",\n",
        "    \"sa\", \"gmbh\", \"ag\", \"nv\", \"bv\", \"plc\", \"limited\",\n",
        "    # Honorifics\n",
        "    \"mr\", \"mrs\", \"ms\", \"dr\", \"prof\",\n",
        "    # Common words\n",
        "    \"the\", \"of\", \"and\", \"for\", \"de\", \"la\", \"el\"\n",
        "}\n",
        "\n",
        "def tokenize(name: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Tokenize a normalized name into words, filtering stopwords and short tokens.\n",
        "    \n",
        "    Splits on whitespace and hyphens, removes tokens shorter than 2 characters,\n",
        "    and filters out common business/legal terms that don't aid matching.\n",
        "    \n",
        "    Args:\n",
        "        name: Normalized name string (already lowercased and cleaned)\n",
        "        \n",
        "    Returns:\n",
        "        List of filtered tokens\n",
        "        \n",
        "    Examples:\n",
        "        >>> tokenize(\"john doe\")\n",
        "        ['john', 'doe']\n",
        "        \n",
        "        >>> tokenize(\"acme corporation ltd\")\n",
        "        ['acme']\n",
        "        \n",
        "        >>> tokenize(\"al-qaida\")\n",
        "        ['al', 'qaida']\n",
        "    \"\"\"\n",
        "    if not name:\n",
        "        return []\n",
        "    \n",
        "    # Split on whitespace and hyphens\n",
        "    tokens = [t for t in re.split(r'[\\s-]+', name) if t]\n",
        "    \n",
        "    # Filter: length >= 2 and not in stopwords\n",
        "    filtered = [t for t in tokens if len(t) >= 2 and t not in STOPWORDS]\n",
        "    \n",
        "    return filtered\n",
        "\n",
        "# Test tokenization function\n",
        "print(\"Testing tokenization:\\n\")\n",
        "test_names = [\n",
        "    \"john doe\",\n",
        "    \"acme corporation ltd\",\n",
        "    \"al-qaida\",\n",
        "    \"banco nacional de cuba\",\n",
        "    \"mr jose maria obrien\",\n",
        "    \"china telecom co ltd\"\n",
        "]\n",
        "\n",
        "for name in test_names:\n",
        "    tokens = tokenize(name)\n",
        "    print(f\"  '{name}' → {tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create Canonical Name Forms\n",
        "\n",
        "We apply tokenization to all normalized names and create three canonical representations for fuzzy matching:\n",
        "\n",
        "1. **name_tokens**: List of filtered tokens for analysis\n",
        "2. **name_sorted**: Tokens sorted alphabetically (for token_sort_ratio matching)\n",
        "3. **name_set**: Space-joined unique tokens (for token_set_ratio matching)\n",
        "\n",
        "These forms enable RapidFuzz to perform robust similarity scoring that handles word order variations, duplicates, and partial matches effectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizing sanctions index...\n",
            "Tokenization complete\n",
            "\n",
            "Sample canonical forms:\n",
            "\n",
            "Original:    'AEROCARIBBEAN AIRLINES'\n",
            "Normalized:  'aerocaribbean airlines'\n",
            "Tokens:      ['aerocaribbean', 'airlines']\n",
            "Sorted:      'aerocaribbean airlines'\n",
            "Set:         'aerocaribbean airlines'\n",
            "\n",
            "Original:    'SHINING PATH'\n",
            "Normalized:  'shining path'\n",
            "Tokens:      ['shining', 'path']\n",
            "Sorted:      'path shining'\n",
            "Set:         'path shining'\n",
            "\n",
            "Original:    'HATKAEW COMPANY LTD.'\n",
            "Normalized:  'hatkaew company ltd'\n",
            "Tokens:      ['hatkaew']\n",
            "Sorted:      'hatkaew'\n",
            "Set:         'hatkaew'\n",
            "\n",
            "Original:    'SHAMALOV, Kirill Nikolaevich'\n",
            "Normalized:  'shamalov kirill nikolaevich'\n",
            "Tokens:      ['shamalov', 'kirill', 'nikolaevich']\n",
            "Sorted:      'kirill nikolaevich shamalov'\n",
            "Set:         'kirill nikolaevich shamalov'\n",
            "\n",
            "Original:    'JOINT STOCK COMPANY RESEARCH INSTITUTE OF ELECTRONIC AND MECHANICAL DEVICES'\n",
            "Normalized:  'joint stock company research institute of electronic and mechanical devices'\n",
            "Tokens:      ['joint', 'stock', 'research', 'institute', 'electronic', 'mechanical', 'devices']\n",
            "Sorted:      'devices electronic institute joint mechanical research stock'\n",
            "Set:         'devices electronic institute joint mechanical research stock'\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Apply tokenization to all normalized names\n",
        "print(\"Tokenizing sanctions index...\")\n",
        "sanctions_index['name_tokens'] = sanctions_index['name_norm'].apply(tokenize)\n",
        "\n",
        "# Create sorted token string (for token_sort_ratio)\n",
        "sanctions_index['name_sorted'] = sanctions_index['name_tokens'].apply(\n",
        "    lambda tokens: ' '.join(sorted(tokens))\n",
        ")\n",
        "\n",
        "# Create unique token set string (for token_set_ratio)\n",
        "sanctions_index['name_set'] = sanctions_index['name_tokens'].apply(\n",
        "    lambda tokens: ' '.join(sorted(set(tokens)))\n",
        ")\n",
        "\n",
        "print(f\"Tokenization complete\")\n",
        "print(f\"\\nSample canonical forms:\\n\")\n",
        "\n",
        "# Show examples of canonical forms\n",
        "sample_indices = [0, 100, 1000, 5000, 10000]\n",
        "for idx in sample_indices:\n",
        "    if idx < len(sanctions_index):\n",
        "        row = sanctions_index.iloc[idx]\n",
        "        print(f\"Original:    '{row['name']}'\")\n",
        "        print(f\"Normalized:  '{row['name_norm']}'\")\n",
        "        print(f\"Tokens:      {row['name_tokens']}\")\n",
        "        print(f\"Sorted:      '{row['name_sorted']}'\")\n",
        "        print(f\"Set:         '{row['name_set']}'\")\n",
        "        print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tokenization Validation\n",
        "\n",
        "We validate the tokenization quality to ensure our canonical forms are suitable for fuzzy matching. Key checks include:\n",
        "- **Empty token handling**: Identify names that produce no tokens after filtering\n",
        "- **Stopword effectiveness**: Verify that stopword removal reduces noise without losing critical information\n",
        "- **Token distribution**: Analyze token counts to understand name complexity\n",
        "\n",
        "Names with empty tokens after filtering may require special handling or indicate data quality issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Check 1: Empty Tokens\n",
            " Records with empty tokens: 10\n",
            "\n",
            "Sample records with empty tokens:\n",
            " Original: 'T.E.G. LIMITED' | Normalized: 't e g limited'\n",
            " Original: 'J & E S. DE R.L. DE C.V.' | Normalized: 'j e s de r l de c v'\n",
            " Original: 'K M A' | Normalized: 'k m a'\n",
            " Original: 'S.A.S. E.U.' | Normalized: 's a s e u'\n",
            " Original: 'T.D.G.' | Normalized: 't d g'\n",
            "\n",
            "[INFO] These names contain only stopwords or short tokens\n",
            "\n",
            "Validation Check 2: Token Count Distribution\n",
            " Mean tokens per name: 3.21\n",
            " Median tokens per name: 3\n",
            " Max tokens per name: 21\n",
            "\n",
            "Distribution:\n",
            " 0 tokens:     10 names (  0.0%)\n",
            " 1 tokens:  2,369 names (  6.0%)\n",
            " 2 tokens: 11,228 names ( 28.5%)\n",
            " 3 tokens: 13,807 names ( 35.1%)\n",
            " 4 tokens:  6,227 names ( 15.8%)\n",
            " 5 tokens:  2,748 names (  7.0%)\n",
            " 6 tokens:  1,404 names (  3.6%)\n",
            " 7 tokens:    753 names (  1.9%)\n",
            " 8 tokens:    361 names (  0.9%)\n",
            " 9 tokens:    206 names (  0.5%)\n",
            "\n",
            "Validation Check 3: Stopword Removal Effectiveness\n",
            " Sample of 1,000 names:\n",
            "  Names with stopwords: 194 (19.4%)\n",
            "  Total stopwords removed: 266\n",
            "  Avg stopwords per affected name: 1.37\n",
            "  Stopword filtering is active and reducing noise\n"
          ]
        }
      ],
      "source": [
        "# Validation Check 1: Empty tokens after filtering\n",
        "empty_tokens = sanctions_index[sanctions_index['name_tokens'].apply(len) == 0]\n",
        "print(f\"Validation Check 1: Empty Tokens\")\n",
        "print(f\" Records with empty tokens: {len(empty_tokens)}\")\n",
        "\n",
        "if len(empty_tokens) > 0:\n",
        "    print(f\"\\nSample records with empty tokens:\")\n",
        "    for idx in empty_tokens.head(5).index:\n",
        "        row = sanctions_index.loc[idx]\n",
        "        print(f\" Original: '{row['name']}' | Normalized: '{row['name_norm']}'\")\n",
        "    print(f\"\\n[INFO] These names contain only stopwords or short tokens\")\n",
        "else:\n",
        "    print(f\"PASS - All names have at least one token\\n\")\n",
        "\n",
        "# Validation Check 2: Token count distribution\n",
        "print(f\"\\nValidation Check 2: Token Count Distribution\")\n",
        "token_counts = sanctions_index['name_tokens'].apply(len)\n",
        "print(f\" Mean tokens per name: {token_counts.mean():.2f}\")\n",
        "print(f\" Median tokens per name: {token_counts.median():.0f}\")\n",
        "print(f\" Max tokens per name: {token_counts.max()}\")\n",
        "print(f\"\\nDistribution:\")\n",
        "for count, freq in token_counts.value_counts().sort_index().head(10).items():\n",
        "    pct = (freq / len(sanctions_index)) * 100\n",
        "    print(f\" {count} tokens: {freq:>6,} names ({pct:>5.1f}%)\")\n",
        "\n",
        "# Validation Check 3: Stopword removal effectiveness\n",
        "print(f\"\\nValidation Check 3: Stopword Removal Effectiveness\")\n",
        "# Count how many names had stopwords removed\n",
        "names_with_stopwords = 0\n",
        "total_stopwords_removed = 0\n",
        "\n",
        "for idx, row in sanctions_index.head(1000).iterrows():\n",
        "    # Re-tokenize without stopword filter to compare\n",
        "    raw_tokens = [t for t in re.split(r'[\\s-]+', row['name_norm']) if t and len(t) >= 2]\n",
        "    filtered_tokens = row['name_tokens']\n",
        "    \n",
        "    removed = len(raw_tokens) - len(filtered_tokens)\n",
        "    if removed > 0:\n",
        "        names_with_stopwords += 1\n",
        "        total_stopwords_removed += removed\n",
        "\n",
        "print(f\" Sample of 1,000 names:\")\n",
        "print(f\"  Names with stopwords: {names_with_stopwords} ({names_with_stopwords/10:.1f}%)\")\n",
        "print(f\"  Total stopwords removed: {total_stopwords_removed}\")\n",
        "print(f\"  Avg stopwords per affected name: {total_stopwords_removed/names_with_stopwords if names_with_stopwords > 0 else 0:.2f}\")\n",
        "print(f\"  Stopword filtering is active and reducing noise\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save Enhanced Sanctions Index\n",
        "\n",
        "We update the sanctions index artifact to include the tokenized canonical forms. This enriched index serves as the foundation for all subsequent fuzzy matching operations, including candidate generation (blocking) and similarity scoring.\n",
        "\n",
        "Saving the tokenized forms ensures:\n",
        "- **Performance**: Tokenization is computed once, not repeated for every screening request\n",
        "- **Reproducibility**: Exact token transformations are preserved for audit and debugging\n",
        "- **Pipeline efficiency**: Downstream steps (blocking, scoring) can load pre-processed data directly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated sanctions index: /Users/joekariuki/Documents/Devbrew/research/devbrew-payments-fraud-sanctions/packages/models/sanctions_index.parquet\n",
            " - Shape: (39350, 13)\n",
            " - Columns: ['uid', 'ent_num', 'name', 'name_norm', 'name_type', 'entity_type', 'program', 'country', 'remarks', 'source', 'name_tokens', 'name_sorted', 'name_set']\n",
            " - Size: 4692.3 KB\n",
            "\n",
            "Updated metadata: /Users/joekariuki/Documents/Devbrew/research/devbrew-payments-fraud-sanctions/packages/models/sanctions_index_metadata.json\n",
            "Enhanced Sanctions Index Ready\n",
            " - 39,350 records with tokenized canonical forms\n",
            " - Avg 3.21 tokens per name\n"
          ]
        }
      ],
      "source": [
        "# Update sanctions index with tokenized columns\n",
        "sanctions_index_path = MODELS_DIR / \"sanctions_index.parquet\"\n",
        "sanctions_index.to_parquet(sanctions_index_path, index=False)\n",
        "\n",
        "print(f\"Updated sanctions index: {sanctions_index_path}\")\n",
        "print(f\" - Shape: {sanctions_index.shape}\")\n",
        "print(f\" - Columns: {list(sanctions_index.columns)}\")\n",
        "print(f\" - Size: {sanctions_index_path.stat().st_size / 1024:.1f} KB\")\n",
        "\n",
        "# Update metadata to reflect tokenization\n",
        "metadata = {\n",
        "    \"created_at\": pd.Timestamp.now().isoformat(),\n",
        "    \"total_records\": len(sanctions_index),\n",
        "    \"unique_entities\": sanctions_index['ent_num'].nunique(),\n",
        "    \"sources\": sanctions_index['source'].value_counts().to_dict(),\n",
        "    \"tokenization\": {\n",
        "        \"stopwords_count\": len(STOPWORDS),\n",
        "        \"stopwords\": sorted(list(STOPWORDS)),\n",
        "        \"empty_token_records\": len(sanctions_index[sanctions_index['name_tokens'].apply(len) == 0]),\n",
        "        \"mean_tokens_per_name\": float(sanctions_index['name_tokens'].apply(len).mean()),\n",
        "        \"median_tokens_per_name\": float(sanctions_index['name_tokens'].apply(len).median()),\n",
        "        \"max_tokens_per_name\": int(sanctions_index['name_tokens'].apply(len).max())\n",
        "    },\n",
        "    \"columns\": list(sanctions_index.columns),\n",
        "    \"validation\": {\n",
        "        \"empty_normalized_names\": 0,\n",
        "        \"duplicate_uids\": 0,\n",
        "        \"entity_type_coverage_pct\": float(sanctions_index['entity_type'].notna().mean() * 100),\n",
        "        \"program_coverage_pct\": float(sanctions_index['program'].notna().mean() * 100)\n",
        "    }\n",
        "}\n",
        "\n",
        "metadata_path = MODELS_DIR / \"sanctions_index_metadata.json\"\n",
        "with open(metadata_path, 'w') as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "print(f\"\\nUpdated metadata: {metadata_path}\")\n",
        "print(f\"Enhanced Sanctions Index Ready\")\n",
        "print(f\" - {len(sanctions_index):,} records with tokenized canonical forms\")\n",
        "print(f\" - Avg {metadata['tokenization']['mean_tokens_per_name']:.2f} tokens per name\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Candidate Generation (Blocking)\n",
        "\n",
        "Screening a query name against all 39K+ sanctions records would be computationally expensive. Blocking reduces the search space by creating efficient indices that quickly identify likely candidates based on shared characteristics.\n",
        "\n",
        "Our blocking strategy uses three complementary approaches:\n",
        "- **First token blocking**: Names starting with the same word (e.g., \"John\" → all \"John X\" entries)\n",
        "- **Token count blocking**: Group by name complexity (1-2 tokens, 3-4 tokens, 5+ tokens)\n",
        "- **Initial signature blocking**: Match by initials pattern (e.g., \"j-d\" for \"John Doe\")\n",
        "\n",
        "This multi-index approach ensures high recall (≥99.5%) while dramatically reducing the candidate set that needs fuzzy scoring. For example, screening \"John Doe\" might reduce from 39K candidates to ~200-500 relevant entries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing blocking functions:\n",
            "\n",
            "Tokens: ['john', 'doe']\n",
            " First token: 'john'\n",
            " Bucket: small\n",
            " Initials: 'j-d'\n",
            "\n",
            "Tokens: ['al', 'qaida']\n",
            " First token: 'al'\n",
            " Bucket: small\n",
            " Initials: 'a-q'\n",
            "\n",
            "Tokens: ['banco', 'nacional', 'cuba']\n",
            " First token: 'banco'\n",
            " Bucket: medium\n",
            " Initials: 'b-n-c'\n",
            "\n",
            "Tokens: ['acme']\n",
            " First token: 'acme'\n",
            " Bucket: tiny\n",
            " Initials: 'a'\n",
            "\n",
            "Tokens: []\n",
            " First token: ''\n",
            " Bucket: tiny\n",
            " Initials: ''\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def get_first_token(tokens: List[str]) -> str:\n",
        "    \"\"\"Extract first token for prefix blocking.\"\"\"\n",
        "    return tokens[0] if tokens else \"\"\n",
        "\n",
        "def get_token_count_bucket(tokens: List[str]) -> str:\n",
        "    \"\"\"\n",
        "    Bucket names by token count for length-based blocking.\n",
        "    \n",
        "    Groups:\n",
        "    - \"tiny\": 0-1 tokens\n",
        "    - \"small\": 2 tokens  \n",
        "    - \"medium\": 3-4 tokens\n",
        "    - \"large\": 5+ tokens\n",
        "    \"\"\"\n",
        "    count = len(tokens)\n",
        "    if count <= 1:\n",
        "        return \"tiny\"\n",
        "    elif count == 2:\n",
        "        return \"small\"\n",
        "    elif count <= 4:\n",
        "        return \"medium\"\n",
        "    else:\n",
        "        return \"large\"\n",
        "\n",
        "def get_initials_signature(tokens: List[str]) -> str:\n",
        "    \"\"\"\n",
        "    Create initials signature from first letter of each token.\n",
        "    \n",
        "    Examples:\n",
        "        ['john', 'doe'] → 'j-d'\n",
        "        ['al', 'qaida'] → 'a-q'\n",
        "        ['banco', 'nacional', 'cuba'] → 'b-n-c'\n",
        "    \"\"\"\n",
        "    if not tokens:\n",
        "        return \"\"\n",
        "    return \"-\".join(t[0] for t in tokens if t)\n",
        "\n",
        "# Test blocking functions\n",
        "print(\"Testing blocking functions:\\n\")\n",
        "test_cases = [\n",
        "    ['john', 'doe'],\n",
        "    ['al', 'qaida'],\n",
        "    ['banco', 'nacional', 'cuba'],\n",
        "    ['acme'],\n",
        "    []\n",
        "]\n",
        "\n",
        "for tokens in test_cases:\n",
        "    first = get_first_token(tokens)\n",
        "    bucket = get_token_count_bucket(tokens)\n",
        "    initials = get_initials_signature(tokens)\n",
        "    print(f\"Tokens: {tokens}\")\n",
        "    print(f\" First token: '{first}'\")\n",
        "    print(f\" Bucket: {bucket}\")\n",
        "    print(f\" Initials: '{initials}'\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Apply Blocking Keys\n",
        "\n",
        "We compute blocking keys for all sanctions records and add them as indexed columns. These keys enable fast candidate retrieval during screening operations.\n",
        "\n",
        "Each blocking key creates a different \"view\" of the data:\n",
        "- **first_token**: Groups names by their starting word\n",
        "- **token_bucket**: Groups by name complexity/length\n",
        "- **initials**: Groups by letter pattern (useful for abbreviated names)\n",
        "\n",
        "Multiple blocking strategies increase recall by capturing different matching scenarios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing blocking keys for sanctions index...\n",
            "Blocking keys computed\n",
            "\n",
            "Blocking Key Distributions:\n",
            "\n",
            "First Token Distribution (Top 15):\n",
            " 'al'                : 2,098 ( 5.3%)\n",
            " 'liability'         : 1,101 ( 2.8%)\n",
            " 'joint'             :   781 ( 2.0%)\n",
            " 'jsc'               :   403 ( 1.0%)\n",
            " 'obshchestvo'       :   372 ( 0.9%)\n",
            " 'aktsionernoe'      :   368 ( 0.9%)\n",
            " 'ao'                :   318 ( 0.8%)\n",
            " 'ooo'               :   304 ( 0.8%)\n",
            " 'ep'                :   139 ( 0.4%)\n",
            " 'open'              :   134 ( 0.3%)\n",
            " 'bank'              :   134 ( 0.3%)\n",
            " 'islamic'           :   132 ( 0.3%)\n",
            " 'fu'                :   123 ( 0.3%)\n",
            " 'public'            :   112 ( 0.3%)\n",
            " 'kim'               :   111 ( 0.3%)\n",
            "\n",
            "Token Bucket Distribution:\n",
            "  medium    : 20,034 ( 50.9%)\n",
            "  small     : 11,228 ( 28.5%)\n",
            "  large     :  5,709 ( 14.5%)\n",
            "  tiny      :  2,379 (  6.0%)\n",
            "\n",
            "Initials Signature Distribution (Top 15):\n",
            " 's'                 :   256 ( 0.7%)\n",
            " 'a'                 :   242 ( 0.6%)\n",
            " 's-a'               :   164 ( 0.4%)\n",
            " 't'                 :   157 ( 0.4%)\n",
            " 'a-a'               :   153 ( 0.4%)\n",
            " 'a-s'               :   142 ( 0.4%)\n",
            " 'k-a'               :   139 ( 0.4%)\n",
            " 'a-m'               :   138 ( 0.4%)\n",
            " 'p'                 :   135 ( 0.3%)\n",
            " 'c'                 :   134 ( 0.3%)\n",
            " 'm'                 :   134 ( 0.3%)\n",
            " 'b'                 :   119 ( 0.3%)\n",
            " 'k'                 :   115 ( 0.3%)\n",
            " 'i'                 :   110 ( 0.3%)\n",
            " 'r'                 :   100 ( 0.3%)\n",
            "\n",
            "Sample Blocking Keys:\n",
            "\n",
            "Name: 'AEROCARIBBEAN AIRLINES'\n",
            " Tokens: ['aerocaribbean', 'airlines']\n",
            " First token: 'aerocaribbean'\n",
            " Bucket: small\n",
            " Initials: 'a-a'\n",
            "\n",
            "Name: 'SHINING PATH'\n",
            " Tokens: ['shining', 'path']\n",
            " First token: 'shining'\n",
            " Bucket: small\n",
            " Initials: 's-p'\n",
            "\n",
            "Name: 'HATKAEW COMPANY LTD.'\n",
            " Tokens: ['hatkaew']\n",
            " First token: 'hatkaew'\n",
            " Bucket: tiny\n",
            " Initials: 'h'\n",
            "\n",
            "Name: 'SHAMALOV, Kirill Nikolaevich'\n",
            " Tokens: ['shamalov', 'kirill', 'nikolaevich']\n",
            " First token: 'shamalov'\n",
            " Bucket: medium\n",
            " Initials: 's-k-n'\n"
          ]
        }
      ],
      "source": [
        "# Apply blocking keys to all sanctions records\n",
        "print(\"Computing blocking keys for sanctions index...\")\n",
        "\n",
        "sanctions_index['first_token'] = sanctions_index['name_tokens'].apply(get_first_token)\n",
        "sanctions_index['token_bucket'] = sanctions_index['name_tokens'].apply(get_token_count_bucket)\n",
        "sanctions_index['initials'] = sanctions_index['name_tokens'].apply(get_initials_signature)\n",
        "\n",
        "print(f\"Blocking keys computed\")\n",
        "print(f\"\\nBlocking Key Distributions:\\n\")\n",
        "\n",
        "# Show distribution of blocking keys\n",
        "print(\"First Token Distribution (Top 15):\")\n",
        "first_token_dist = sanctions_index['first_token'].value_counts()\n",
        "for token, count in first_token_dist.head(15).items():\n",
        "    pct = (count / len(sanctions_index)) * 100\n",
        "    token_str = f\"'{token}'\" if token else \"'(empty)'\"\n",
        "    print(f\" {token_str:20s}: {count:>5,} ({pct:>4.1f}%)\")\n",
        "\n",
        "print(\"\\nToken Bucket Distribution:\")\n",
        "bucket_dist = sanctions_index['token_bucket'].value_counts()\n",
        "for bucket, count in bucket_dist.items():\n",
        "    pct = (count / len(sanctions_index)) * 100\n",
        "    print(f\"  {bucket:10s}: {count:>6,} ({pct:>5.1f}%)\")\n",
        "\n",
        "print(\"\\nInitials Signature Distribution (Top 15):\")\n",
        "initials_dist = sanctions_index['initials'].value_counts()\n",
        "for initials, count in initials_dist.head(15).items():\n",
        "    pct = (count / len(sanctions_index)) * 100\n",
        "    initials_str = f\"'{initials}'\" if initials else \"'(empty)'\"\n",
        "    print(f\" {initials_str:20s}: {count:>5,} ({pct:>4.1f}%)\")\n",
        "\n",
        "# Show sample blocking keys\n",
        "print(\"\\nSample Blocking Keys:\")\n",
        "for idx in [0, 100, 1000, 5000]:\n",
        "    if idx < len(sanctions_index):\n",
        "        row = sanctions_index.iloc[idx]\n",
        "        print(f\"\\nName: '{row['name']}'\")\n",
        "        print(f\" Tokens: {row['name_tokens']}\")\n",
        "        print(f\" First token: '{row['first_token']}'\")\n",
        "        print(f\" Bucket: {row['token_bucket']}\")\n",
        "        print(f\" Initials: '{row['initials']}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Build Blocking Indices\n",
        "\n",
        "We create inverted indices that map blocking keys to lists of candidate record indices. These indices enable O(1) lookup of candidates during screening operations.\n",
        "\n",
        "For example:\n",
        "- `first_token_index['john']` → [123, 456, 789, ...] (all records starting with \"john\")\n",
        "- `bucket_index['small']` → [1, 5, 12, ...] (all 2-token names)\n",
        "- `initials_index['j-d']` → [123, 456] (all names with pattern \"j-d\")\n",
        "\n",
        "During screening, we query multiple indices and take the union of candidates to maximize recall while keeping the candidate set manageable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building blocking indices...\n",
            "Blocking indices built\n",
            "\n",
            "Index Statistics:\n",
            "\n",
            "First Token Index:\n",
            " Unique keys: 15,597\n",
            " Avg candidates per key: 2.5\n",
            " Max candidates per key: 2,098\n",
            "\n",
            "Token Bucket Index:\n",
            " Unique keys: 4\n",
            " Avg candidates per key: 9837.5\n",
            " Max candidates per key: 20,034\n",
            "\n",
            "Initials Index:\n",
            " Unique keys: 15,986\n",
            " Avg candidates per key: 2.5\n",
            " Max candidates per key: 256\n",
            "\n",
            "Example Index Lookups:\n",
            "\n",
            "first_token['bank']:\n",
            " Candidates: 134\n",
            " Sample names:\n",
            "  - BANK MARKAZI JOMHOURI ISLAMI IRAN\n",
            "  - BANK MASKAN\n",
            "  - BANK REFAH KARGARAN\n",
            "\n",
            "first_token['john']:\n",
            " Candidates: 1\n",
            " Sample names:\n",
            "  - JOHN, Damion Patrick\n",
            "\n",
            "bucket['medium']:\n",
            " Candidates: 20,034\n",
            " Sample names:\n",
            "  - BANCO NACIONAL DE CUBA\n",
            "  - COMERCIAL DE RODAJES Y MAQUINARIA, S.A.\n",
            "  - COMERCIALIZACION DE PRODUCTOS VARIOS\n",
            "\n",
            "initials['j-d']:\n",
            " Candidates: 5\n",
            " Sample names:\n",
            "  - JOKIC, Dragan\n",
            "  - JSC DRAGA\n",
            "  - JAMA'AT-I-DAWAT\n"
          ]
        }
      ],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def build_blocking_index(df: pd.DataFrame, key_column: str) -> Dict[str, List[int]]:\n",
        "    \"\"\"\n",
        "    Build inverted index mapping blocking keys to record indices.\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame with blocking keys\n",
        "        key_column: Name of column containing blocking keys\n",
        "        \n",
        "    Returns:\n",
        "        Dictionary mapping key values to lists of row indices\n",
        "    \"\"\"\n",
        "    index = defaultdict(list)\n",
        "    for idx, key in enumerate(df[key_column]):\n",
        "        if key:  # Skip empty keys\n",
        "            index[key].append(idx)\n",
        "    return dict(index)\n",
        "\n",
        "print(\"Building blocking indices...\")\n",
        "\n",
        "# Build indices for each blocking strategy\n",
        "first_token_index = build_blocking_index(sanctions_index, 'first_token')\n",
        "bucket_index = build_blocking_index(sanctions_index, 'token_bucket')\n",
        "initials_index = build_blocking_index(sanctions_index, 'initials')\n",
        "\n",
        "print(f\"Blocking indices built\")\n",
        "print(f\"\\nIndex Statistics:\\n\")\n",
        "\n",
        "print(f\"First Token Index:\")\n",
        "print(f\" Unique keys: {len(first_token_index):,}\")\n",
        "print(f\" Avg candidates per key: {np.mean([len(v) for v in first_token_index.values()]):.1f}\")\n",
        "print(f\" Max candidates per key: {max(len(v) for v in first_token_index.values()):,}\")\n",
        "\n",
        "print(f\"\\nToken Bucket Index:\")\n",
        "print(f\" Unique keys: {len(bucket_index):,}\")\n",
        "print(f\" Avg candidates per key: {np.mean([len(v) for v in bucket_index.values()]):.1f}\")\n",
        "print(f\" Max candidates per key: {max(len(v) for v in bucket_index.values()):,}\")\n",
        "\n",
        "print(f\"\\nInitials Index:\")\n",
        "print(f\" Unique keys: {len(initials_index):,}\")\n",
        "print(f\" Avg candidates per key: {np.mean([len(v) for v in initials_index.values()]):.1f}\")\n",
        "print(f\" Max candidates per key: {max(len(v) for v in initials_index.values()):,}\")\n",
        "\n",
        "# Show example lookups\n",
        "print(\"\\nExample Index Lookups:\")\n",
        "\n",
        "example_keys = [\n",
        "    ('first_token', 'bank', first_token_index),\n",
        "    ('first_token', 'john', first_token_index),\n",
        "    ('bucket', 'medium', bucket_index),\n",
        "    ('initials', 'j-d', initials_index)\n",
        "]\n",
        "\n",
        "for index_type, key, index in example_keys:\n",
        "    candidates = index.get(key, [])\n",
        "    print(f\"\\n{index_type}['{key}']:\")\n",
        "    print(f\" Candidates: {len(candidates):,}\")\n",
        "    if candidates:\n",
        "        # Show first 3 candidate names\n",
        "        print(f\" Sample names:\")\n",
        "        for idx in candidates[:3]:\n",
        "            name = sanctions_index.iloc[idx]['name']\n",
        "            print(f\"  - {name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Candidate Retrieval Function\n",
        "\n",
        "We implement the candidate retrieval logic that queries multiple blocking indices and returns the union of candidates. This multi-strategy approach maximizes recall by capturing different matching scenarios.\n",
        "\n",
        "The retrieval strategy:\n",
        "1. Extract blocking keys from query name (first token, bucket, initials)\n",
        "2. Query each index to get candidate lists\n",
        "3. Take union of all candidates (deduplicate)\n",
        "4. Return candidate indices for fuzzy scoring\n",
        "\n",
        "This approach ensures we don't miss potential matches due to variations in name formatting or word order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing candidate retrieval:\n",
            "\n",
            "Query: 'john doe'\n",
            "  Normalized: 'john doe'\n",
            "  Candidates: 11,229\n",
            "  Sample matches:\n",
            "    - AEROCARIBBEAN AIRLINES\n",
            "    - ANGLO-CARIBBEAN CO., LTD.\n",
            "    - BOUTIQUE LA MAISON\n",
            "    - CASA DE CUBA\n",
            "    - CIMEX IBERICA\n",
            "\n",
            "Query: 'bank of china'\n",
            "  Normalized: 'bank of china'\n",
            "  Candidates: 11,329\n",
            "  Sample matches:\n",
            "    - AEROCARIBBEAN AIRLINES\n",
            "    - ANGLO-CARIBBEAN CO., LTD.\n",
            "    - BOUTIQUE LA MAISON\n",
            "    - CASA DE CUBA\n",
            "    - CIMEX IBERICA\n",
            "\n",
            "Query: 'al qaida'\n",
            "  Normalized: 'al qaida'\n",
            "  Candidates: 13,257\n",
            "  Sample matches:\n",
            "    - AEROCARIBBEAN AIRLINES\n",
            "    - ANGLO-CARIBBEAN CO., LTD.\n",
            "    - BOUTIQUE LA MAISON\n",
            "    - CASA DE CUBA\n",
            "    - CIMEX IBERICA\n",
            "\n",
            "Query: 'acme corporation'\n",
            "  Normalized: 'acme corporation'\n",
            "  Candidates: 2,379\n",
            "  Sample matches:\n",
            "    - CECOEX, S.A.\n",
            "    - CIMEX\n",
            "    - CIMEX, S.A.\n",
            "    - COTEI\n",
            "    - CUBAEXPORT\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def get_candidates(\n",
        "    query_name: str,\n",
        "    first_token_idx: Dict[str, List[int]],\n",
        "    bucket_idx: Dict[str, List[int]],\n",
        "    initials_idx: Dict[str, List[int]]\n",
        ") -> List[int]:\n",
        "    \"\"\"\n",
        "    Retrieve candidate indices using multi-strategy blocking.\n",
        "    \n",
        "    Args:\n",
        "        query_name: Normalized query name to screen\n",
        "        first_token_idx: First token blocking index\n",
        "        bucket_idx: Token bucket blocking index\n",
        "        initials_idx: Initials signature blocking index\n",
        "        \n",
        "    Returns:\n",
        "        List of candidate record indices (deduplicated)\n",
        "    \"\"\"\n",
        "    # Tokenize query\n",
        "    query_tokens = tokenize(query_name)\n",
        "    \n",
        "    if not query_tokens:\n",
        "        return []\n",
        "    \n",
        "    # Extract blocking keys from query\n",
        "    query_first = get_first_token(query_tokens)\n",
        "    query_bucket = get_token_count_bucket(query_tokens)\n",
        "    query_initials = get_initials_signature(query_tokens)\n",
        "    \n",
        "    # Collect candidates from all indices\n",
        "    candidates = set()\n",
        "    \n",
        "    # Strategy 1: First token match\n",
        "    if query_first in first_token_idx:\n",
        "        candidates.update(first_token_idx[query_first])\n",
        "    \n",
        "    # Strategy 2: Token bucket match (same complexity)\n",
        "    if query_bucket in bucket_idx:\n",
        "        candidates.update(bucket_idx[query_bucket])\n",
        "    \n",
        "    # Strategy 3: Initials match\n",
        "    if query_initials in initials_idx:\n",
        "        candidates.update(initials_idx[query_initials])\n",
        "    \n",
        "    return sorted(list(candidates))\n",
        "\n",
        "# Test candidate retrieval\n",
        "print(\"Testing candidate retrieval:\\n\")\n",
        "\n",
        "test_queries = [\n",
        "    \"john doe\",\n",
        "    \"bank of china\",\n",
        "    \"al qaida\",\n",
        "    \"acme corporation\"\n",
        "]\n",
        "\n",
        "for query in test_queries:\n",
        "    # Normalize query\n",
        "    query_norm = normalize_text(query)\n",
        "    \n",
        "    # Get candidates\n",
        "    candidates = get_candidates(\n",
        "        query_norm,\n",
        "        first_token_index,\n",
        "        bucket_index,\n",
        "        initials_index\n",
        "    )\n",
        "    \n",
        "    print(f\"Query: '{query}'\")\n",
        "    print(f\"  Normalized: '{query_norm}'\")\n",
        "    print(f\"  Candidates: {len(candidates):,}\")\n",
        "    \n",
        "    # Show sample candidate names\n",
        "    if candidates:\n",
        "        print(f\"  Sample matches:\")\n",
        "        for idx in candidates[:5]:\n",
        "            name = sanctions_index.iloc[idx]['name']\n",
        "            print(f\"    - {name}\")\n",
        "    print()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
