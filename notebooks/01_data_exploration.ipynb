{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30f8a042",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317060cf",
   "metadata": {},
   "source": [
    "#### Purpose: Initial data exploration and quality assessment\n",
    "\n",
    "Author: Devbrew LLC\n",
    "\n",
    "Created: 2025-10-01\n",
    "\n",
    "Last Modified: 2025-10-01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fd7a6f",
   "metadata": {},
   "source": [
    "## Notebook Configuration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fe7d2f",
   "metadata": {},
   "source": [
    "#### Load packages and configure environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5469aaeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Environment configured successfully\n",
      "  - pandas: 2.3.3\n",
      "  - numpy: 2.3.3\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "# Plotting configuration\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"✅ Environment configured successfully\")\n",
    "print(f\"  - pandas: {pd.__version__}\")\n",
    "print(f\"  - numpy: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef128afa",
   "metadata": {},
   "source": [
    "### Path Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3207132f",
   "metadata": {},
   "source": [
    "Sets up project directory structure and validates data availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d020dd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Availability Check:\n",
      "------------------------------------------------------------\n",
      "IEEE Train Transaction: ✅\n",
      "IEEE Train Identity: ✅\n",
      "IEEE Test Transaction: ✅\n",
      "IEEE Test Identity: ✅\n",
      "PaySim: ✅\n",
      "OFAC SDN: ✅\n",
      "OFAC SDN ADD: ✅\n",
      "OFAC SDN ALT: ✅\n",
      "OFAC SDN COMMENTS: ✅\n",
      "OFAC Consolidated: ✅\n",
      "OFAC Consolidated ADD: ✅\n",
      "OFAC Consolidated ALT: ✅\n",
      "OFAC Consolidated COMMENTS: ✅\n",
      "✅ All required datasets are found.\n"
     ]
    }
   ],
   "source": [
    "# Project paths\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "DATA_DIR = PROJECT_ROOT / \"data_catalog\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "NOTEBOOKS_DIR = PROJECT_ROOT / \"notebooks\"\n",
    "\n",
    "# Dataset paths\n",
    "IEEE_CIS_DIR = DATA_DIR / \"ieee-fraud\" # IEEE-CIS Fraud Detection Dataset\n",
    "PAYSIM_DIR = DATA_DIR / \"paysim\" # PaySim Dataset\n",
    "OFAC_DIR = DATA_DIR / \"ofac\" # OFAC Sanctions Dataset\n",
    "\n",
    "# Create output directiories\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# Validate data availability\n",
    "def validate_data_path() -> dict:\n",
    "    \"\"\"Validate that required datasets exists\"\"\"\n",
    "    paths_status = {\n",
    "        'IEEE Train Transaction': (IEEE_CIS_DIR / \"train_transaction.csv\").exists(),\n",
    "        'IEEE Train Identity': (IEEE_CIS_DIR / \"train_identity.csv\").exists(),\n",
    "        'IEEE Test Transaction': (IEEE_CIS_DIR / \"test_transaction.csv\").exists(),\n",
    "        'IEEE Test Identity': (IEEE_CIS_DIR / \"test_identity.csv\").exists(),\n",
    "        'PaySim': (PAYSIM_DIR / \"PS_20174392719_1491204439457_log.csv\").exists(),\n",
    "        'OFAC SDN': (OFAC_DIR / \"sdn\" / \"sdn.csv\").exists(),\n",
    "        'OFAC SDN ADD': (OFAC_DIR / \"sdn\" / \"add.csv\").exists(),\n",
    "        'OFAC SDN ALT': (OFAC_DIR / \"sdn\" / \"alt.csv\").exists(),\n",
    "        'OFAC SDN COMMENTS': (OFAC_DIR / \"sdn\" / \"sdn_comments.csv\").exists(),\n",
    "        'OFAC Consolidated': (OFAC_DIR / \"consolidated\" / \"cons_prim.csv\").exists(),\n",
    "        'OFAC Consolidated ADD': (OFAC_DIR / \"consolidated\" / \"cons_add.csv\").exists(),\n",
    "        'OFAC Consolidated ALT': (OFAC_DIR / \"consolidated\" / \"cons_alt.csv\").exists(),\n",
    "        'OFAC Consolidated COMMENTS': (OFAC_DIR / \"consolidated\" / \"cons_comments.csv\").exists(),\n",
    "    }\n",
    "\n",
    "    print(\"Data Availability Check:\")\n",
    "    print(\"-\" * 60)\n",
    "    for name, exists in paths_status.items():\n",
    "        status = \"✅\" if exists else \"❌\"\n",
    "        print(f\"{name}: {status}\")\n",
    "\n",
    "    all_exist = all(paths_status.values())\n",
    "    if not all_exist:\n",
    "        print(\"\\n⚠️  Warning: Some datasets are missing. Check data_catalog/README.md\")\n",
    "    else: \n",
    "        print(\"✅ All required datasets are found.\")\n",
    "\n",
    "    return paths_status    \n",
    "\n",
    "paths_status = validate_data_path()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a0efc7",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "Reusable utilities for data loading, analysis, and visualization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b9e3f6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb47b290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "def load_dataset(\n",
    "    file_path: Path,\n",
    "    nrows: Optional[int] = None,\n",
    "    parse_dates: Optional[list] = None\n",
    ") -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Load CSV dataset with error handling and logging.\n",
    "\n",
    "    Args:\n",
    "        file_path: Path to CSV file\n",
    "        nrows: Number of rows to load (None = all)\n",
    "        parse_dates: Columns to parse as datetime\n",
    "    \n",
    "    Returns:\n",
    "        Dataframe if successful, None if error\n",
    "\n",
    "    Example:\n",
    "        >>> df = load_dataset(IEEE_DIR / \"train_transaction.csv\", nrows=10000)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, nrows=nrows, parse_dates=parse_dates)\n",
    "        print(f\"✅ Loaded dataset {file_path.name}\")\n",
    "        print(f\"Shape: {df.shape[0]:,} rows x {df.shape[1]:,} columns\")\n",
    "        print(f\"Memory: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ File not found: {file_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading {file_path.name}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Analyze data quality\n",
    "def analyze_data_quality(df: pd.DataFrame, name: str) -> dict:\n",
    "    \"\"\"\n",
    "    Comprehensive data quality assessment.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame to analyze\n",
    "        name: Dataset name for reporting\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of quality metrics\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(f\"Data Quality Report: {name}\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "        \n",
    "\n",
    "    # Basic info\n",
    "    n_rows, n_cols = df.shape\n",
    "    memory_mb = df.memory_usage(deep=True).sum() / 1024**2\n",
    "\n",
    "    print(f\"\\n📊 Dataset Overview:\")\n",
    "    print(f\"  • Rows: {n_rows:,}\")\n",
    "    print(f\"  • Columns: {n_cols}\")\n",
    "    print(f\"  • Memory: {memory_mb:.2f} MB\")\n",
    "\n",
    "    # Data types\n",
    "    print(f\"\\n📋 Column Types:\")\n",
    "    dtype_counts = df.dtypes.value_counts()\n",
    "    for dtype, count in dtype_counts.items():\n",
    "        print(f\"  • {dtype}: {count} columns\")\n",
    "    \n",
    "    # Missing values\n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = (missing / len(df) * 100)\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Count': missing[missing > 0],\n",
    "        'Percent': missing_pct[missing > 0]\n",
    "    }).sort_values('Percent', ascending=False)\n",
    "\n",
    "    print(f\"\\nMissing Values:\")\n",
    "    if len(missing_df) > 0:\n",
    "        print(f\"  • Total missing values: {missing_df['Count'].sum():,}\")\n",
    "        print(f\"  • Columns with missing values: {len(missing_df)}/{n_cols}\")\n",
    "        print(f\"  • Top 5 missing columns:\")\n",
    "        for col, row in missing_df.head(5).iterrows():\n",
    "            print(f\" - {col}: {row['Count']:,} ({row['Percent']:.2f}%)\")\n",
    "    else:\n",
    "        print(\"  • No missing values ✅\")\n",
    "\n",
    "    # Duplicate\n",
    "    n_duplicates = df.duplicated().sum()\n",
    "    dup_pct = (n_duplicates / len(df) * 100)\n",
    "    print(f\"\\n🔄 Duplicates:\")\n",
    "    print(f\"  • Duplicate rows: {n_duplicates:,} ({dup_pct:.2f}%)\")\n",
    "\n",
    "    # Return metrics for programmtic use\n",
    "    return {\n",
    "        'n_rows': n_rows,\n",
    "        'n_cols': n_cols,\n",
    "        'memory_mb': memory_mb,\n",
    "        'missing_cols': len(missing_df),\n",
    "        'missing_pct_max': missing_pct.max() if len(missing_pct) > 0 else 0,\n",
    "        'n_duplicates': n_duplicates,\n",
    "    }   \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c5b06d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded dataset train_transaction.csv\n",
      "Shape: 590,540 rows x 394 columns\n",
      "Memory: 2100.70 MB\n"
     ]
    }
   ],
   "source": [
    "# Test load_dataset\n",
    "train_df = load_dataset(IEEE_CIS_DIR / \"train_transaction.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1900bffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Data Quality Report: train\n",
      "======================================================================\n",
      "\n",
      "📊 Dataset Overview:\n",
      "  • Rows: 590,540\n",
      "  • Columns: 394\n",
      "  • Memory: 2100.70 MB\n",
      "\n",
      "📋 Column Types:\n",
      "  • float64: 376 columns\n",
      "  • object: 14 columns\n",
      "  • int64: 4 columns\n",
      "\n",
      "Missing Values:\n",
      "  • Total missing values: 95,566,686\n",
      "  • Columns with missing values: 374/394\n",
      "  • Top 5 missing columns:\n",
      " - dist2: 552,913.0 (93.63%)\n",
      " - D7: 551,623.0 (93.41%)\n",
      " - D13: 528,588.0 (89.51%)\n",
      " - D14: 528,353.0 (89.47%)\n",
      " - D12: 525,823.0 (89.04%)\n",
      "\n",
      "🔄 Duplicates:\n",
      "  • Duplicate rows: 0 (0.00%)\n",
      "{'n_rows': 590540, 'n_cols': 394, 'memory_mb': np.float64(2100.701410293579), 'missing_cols': 374, 'missing_pct_max': np.float64(93.62837403054831), 'n_duplicates': np.int64(0)}\n"
     ]
    }
   ],
   "source": [
    "# Test analyze_data_quality\n",
    "data_quality = analyze_data_quality(train_df, \"train\")\n",
    "print(data_quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df964ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
