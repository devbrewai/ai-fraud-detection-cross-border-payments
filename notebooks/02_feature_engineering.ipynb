{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f6cb0f0",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "- **Purpose:** Missing value handling and feature engineering for fraud detection  \n",
    "- **Author:** Devbrew LLC  \n",
    "- **Last Updated:** October 18, 2025  \n",
    "- **Status:** In Progress  \n",
    "- **License:** Apache 2.0 (Code) | Non-commercial (Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6977d399",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Dataset License Notice\n",
    "\n",
    "This notebook uses the **IEEE-CIS Fraud Detection dataset** from Kaggle.\n",
    "\n",
    "**Dataset License:** Non-commercial research use only\n",
    "- You must download the dataset yourself from [Kaggle IEEE-CIS Competition](https://www.kaggle.com/c/ieee-fraud-detection)\n",
    "- You must accept the competition rules before downloading\n",
    "- Cannot be used for commercial purposes\n",
    "- Cannot redistribute the raw dataset\n",
    "\n",
    "**Setup Instructions:** See [`../data_catalog/README.md`](../data_catalog/README.md) for download instructions.\n",
    "\n",
    "**Code License:** This notebook's code is licensed under Apache 2.0 (open source).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a939dbf",
   "metadata": {},
   "source": [
    "## Notebook Configuration\n",
    "\n",
    "### Environment Setup\n",
    "\n",
    "We configure the Python environment with standardized settings, import required libraries, and set a fixed random seed for reproducibility. This ensures consistent results across runs and enables reliable experimentation.\n",
    "\n",
    "These settings establish the foundation for all feature engineering operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56e08b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Environment configurated successfully\n",
      "pandas: 2.3.3\n",
      "numpy: 2.3.3\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.float_format\", '{:.2f}'.format)\n",
    "\n",
    "# Plotting configuration\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "plt.rcParams[\"font.size\"] = 10\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"\\nEnvironment configurated successfully\")\n",
    "print(f\"pandas: {pd.__version__}\")\n",
    "print(f\"numpy: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62921305",
   "metadata": {},
   "source": [
    "### Path Configuration\n",
    "\n",
    "We define the project directory structure and validate that required processed data from the exploration phase exists. The validation ensures we have the necessary inputs before proceeding with feature engineering.\n",
    "\n",
    "This configuration pattern ensures we can locate all required data artifacts from previous pipeline stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0393195d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Availability Check:\n",
      " • IEEE Train Transaction: Found\n",
      " • IEEE Train Identity: Found\n",
      "\n",
      "All required datasets are available\n"
     ]
    }
   ],
   "source": [
    "# Project paths\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "DATA_DIR = PROJECT_ROOT / \"data_catalog\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "IEEE_CIS_DIR = DATA_DIR / \"ieee-fraud\"\n",
    "NOTEBOOKS_DIR = PROJECT_ROOT / \"notebooks\"\n",
    "\n",
    "# Ensure processed directory exists\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Validate required data\n",
    "def validate_required_data() -> dict:\n",
    "    \"\"\"Validate that required datasets exist before feature engineering\"\"\"\n",
    "    paths_status = {\n",
    "        'IEEE Train Transaction:': (IEEE_CIS_DIR / 'train_transaction.csv').exists(),\n",
    "        'IEEE Train Identity:': (IEEE_CIS_DIR / 'train_identity.csv').exists(),\n",
    "    }\n",
    "\n",
    "    print(\"\\nData Availability Check:\")\n",
    "    for name, exists in paths_status.items():\n",
    "        status = \"Found\" if exists else \"Missing\"\n",
    "        print(f\" • {name} {status}\")\n",
    "    \n",
    "    all_exist = all(paths_status.values())\n",
    "    if not all_exist:\n",
    "        print(\"\\n[WARNING] Some datasets are missing. Check data_catalog/README.md for instructions\")\n",
    "    else:\n",
    "        print(\"\\nAll required datasets are available\")\n",
    "    \n",
    "    return paths_status\n",
    "    \n",
    "path_status = validate_required_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b070f845",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
