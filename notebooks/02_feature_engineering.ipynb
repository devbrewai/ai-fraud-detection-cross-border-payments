{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f6cb0f0",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "- **Purpose:** Missing value handling and feature engineering for fraud detection  \n",
    "- **Author:** Devbrew LLC  \n",
    "- **Last Updated:** October 18, 2025  \n",
    "- **Status:** In Progress  \n",
    "- **License:** Apache 2.0 (Code) | Non-commercial (Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6977d399",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Dataset License Notice\n",
    "\n",
    "This notebook uses the **IEEE-CIS Fraud Detection dataset** from Kaggle.\n",
    "\n",
    "**Dataset License:** Non-commercial research use only\n",
    "- You must download the dataset yourself from [Kaggle IEEE-CIS Competition](https://www.kaggle.com/c/ieee-fraud-detection)\n",
    "- You must accept the competition rules before downloading\n",
    "- Cannot be used for commercial purposes\n",
    "- Cannot redistribute the raw dataset\n",
    "\n",
    "**Setup Instructions:** See [`../data_catalog/README.md`](../data_catalog/README.md) for download instructions.\n",
    "\n",
    "**Code License:** This notebook's code is licensed under Apache 2.0 (open source).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a939dbf",
   "metadata": {},
   "source": [
    "## Notebook Configuration\n",
    "\n",
    "### Environment Setup\n",
    "\n",
    "We configure the Python environment with standardized settings, import required libraries, and set a fixed random seed for reproducibility. This ensures consistent results across runs and enables reliable experimentation.\n",
    "\n",
    "These settings establish the foundation for all feature engineering operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56e08b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Environment configurated successfully\n",
      "pandas: 2.3.3\n",
      "numpy: 2.3.3\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.float_format\", '{:.2f}'.format)\n",
    "\n",
    "# Plotting configuration\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "plt.rcParams[\"font.size\"] = 10\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"\\nEnvironment configurated successfully\")\n",
    "print(f\"pandas: {pd.__version__}\")\n",
    "print(f\"numpy: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62921305",
   "metadata": {},
   "source": [
    "### Path Configuration\n",
    "\n",
    "We define the project directory structure and validate that required processed data from the exploration phase exists. The validation ensures we have the necessary inputs before proceeding with feature engineering.\n",
    "\n",
    "This configuration pattern ensures we can locate all required data artifacts from previous pipeline stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0393195d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Availability Check:\n",
      " • IEEE Train Transaction: Found\n",
      " • IEEE Train Identity: Found\n",
      "\n",
      "All required datasets are available\n"
     ]
    }
   ],
   "source": [
    "# Project paths\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "DATA_DIR = PROJECT_ROOT / \"data_catalog\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "IEEE_CIS_DIR = DATA_DIR / \"ieee-fraud\"\n",
    "NOTEBOOKS_DIR = PROJECT_ROOT / \"notebooks\"\n",
    "\n",
    "# Ensure processed directory exists\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Validate required data\n",
    "def validate_required_data() -> dict:\n",
    "    \"\"\"Validate that required datasets exist before feature engineering\"\"\"\n",
    "    paths_status = {\n",
    "        'IEEE Train Transaction:': (IEEE_CIS_DIR / 'train_transaction.csv').exists(),\n",
    "        'IEEE Train Identity:': (IEEE_CIS_DIR / 'train_identity.csv').exists(),\n",
    "    }\n",
    "\n",
    "    print(\"\\nData Availability Check:\")\n",
    "    for name, exists in paths_status.items():\n",
    "        status = \"Found\" if exists else \"Missing\"\n",
    "        print(f\" • {name} {status}\")\n",
    "    \n",
    "    all_exist = all(paths_status.values())\n",
    "    if not all_exist:\n",
    "        print(\"\\n[WARNING] Some datasets are missing. Check data_catalog/README.md for instructions\")\n",
    "    else:\n",
    "        print(\"\\nAll required datasets are available\")\n",
    "    \n",
    "    return paths_status\n",
    "    \n",
    "path_status = validate_required_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977a6652",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "We define reusable utilities for missing value analysis, imputation strategies, and feature engineering operations. These functions implement error handling, type hints, and standardized output formats following production best practices.\n",
    "\n",
    "These utilities form the foundation for all feature engineering operations and enable reproducible, maintainable code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b070f845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Helper functions loaded\n"
     ]
    }
   ],
   "source": [
    "def analyze_missing_values(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Comprehensive missing value analysis with categoriazation.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame to analyze\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with missing value statistics\n",
    "    \"\"\"\n",
    "    missing_pct = (df.isnull().sum() / len(df) * 100).sort_values(ascending=False)\n",
    "    missing_summary = pd.DataFrame({\n",
    "        \"column\": missing_pct.index,\n",
    "        \"missing_pct\": missing_pct.values,\n",
    "        \"missing_count\": df.isnull().sum().values\n",
    "    })\n",
    "\n",
    "    # Categorize by severity\n",
    "    missing_summary['category'] = pd.cut(\n",
    "        missing_summary['missing_pct'],\n",
    "        bins=[0.1, 0, 50, 90, 100],\n",
    "        labels=['none', 'low', 'medium', 'high']\n",
    "    )\n",
    "    \n",
    "    return missing_summary\n",
    "\n",
    "def apply_missing_value_strategy(\n",
    "    df: pd.DataFrame,\n",
    "    drop_threshold: float = 90.0,\n",
    "    ) -> Tuple[pd.DataFrame, List[str]]:\n",
    "    \"\"\"\n",
    "    Apply missing value handling strategy\n",
    "\n",
    "    Strategy:\n",
    "    - Drop columns with > 90% missing values\n",
    "    - Impute numeric columns with median\n",
    "    - Impute categorical columns with mode or \"Unknown\"\n",
    "    \n",
    "\n",
    "    Args:\n",
    "        df: DataFrame to process\n",
    "        drop_threshold: Percentage threshold for dropping columns\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (cleaned_df, dropped_columns)\n",
    "    \"\"\"\n",
    "    print(f\"\\nApplying Missing Value Strategy\\n\")\n",
    "\n",
    "    # Analyze missing values\n",
    "    missing_summary = analyze_missing_values(df)\n",
    "   \n",
    "    # Identity columns to drop\n",
    "    cols_to_drop = missing_summary[missing_summary['missing_pct'] > drop_threshold]['column'].tolist()\n",
    "\n",
    "    print(f\"Dropping {len(cols_to_drop)} columns with > {drop_threshold}% missing values: {cols_to_drop}\")\n",
    "    df_clean = df.drop(columns=cols_to_drop)\n",
    "\n",
    "    # Seperate numeric and categorical columns\n",
    "    numeric_cols = df_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_cols = df_clean.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "    # Remove target if present\n",
    "    if 'isFraud' in numeric_cols:\n",
    "        numeric_cols.remove('isFraud')\n",
    "    \n",
    "\n",
    "    # Impute numeric with median\n",
    "    print(f\"\\nImputing {len([col for col in numeric_cols if df_clean[col].isnull().any()])} numeric columns with median\")\n",
    "\n",
    "    for col in numeric_cols:\n",
    "        if df_clean[col].isnull().any():\n",
    "            median_val = df_clean[col].median()\n",
    "            df_clean[col].fillna(median_val, inplace=True)\n",
    "    \n",
    "    # Impute categorical with mode or \"Unknown\"\n",
    "    print(f\"\\nImputing {len([col for col in categorical_cols if df_clean[col].isnull().any()])} categorical columns with mode or 'Unknown'\")\n",
    "    for col in categorical_cols:\n",
    "        if df_clean[col].isnull().any():\n",
    "            mode_val = df_clean[col].mode()\n",
    "            if len(mode_val) > 1:\n",
    "                df_clean[col].fillna(mode_val[0], inplace=True)\n",
    "            else:\n",
    "                df_clean[col].fillna(\"Unknown\", inplace=True)\n",
    "    \n",
    "    # Verify\n",
    "    remaining_missing = df_clean.isnull().sum().sum()\n",
    "    print(f\"\\nMissing values after strategy: {remaining_missing}\")\n",
    "    print(f\"Final shape: {df_clean.shape}\")\n",
    "\n",
    "    return df_clean, cols_to_drop\n",
    "\n",
    "def calculate_velocity_features(\n",
    "    df: pd.DataFrame,\n",
    "    group_col: str,\n",
    "    time_col: str,\n",
    "    windows: List[int] = [3000, 86400],\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate transaction velocity features (count in time window).\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame sorted by time\n",
    "        group_col: Column to group by (e.g. user_id)\n",
    "        time_col: Column containing transaction timestamps\n",
    "        windows: Time windows in seconds [1h=3600, 24h=86400]\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with velocity columns added\n",
    "    \"\"\"\n",
    "    print(f\"\\nCalucalting Velocity Features for {group_col}\\n\")\n",
    "\n",
    "    df = df.sort_values([group_col, time_col]).reset_index(drop=True)\n",
    "    \n",
    "    for window in windows:\n",
    "        window_name = f\"{window/3600}h\" if window >= 3600 else f\"{window}s\"\n",
    "        col_name = f'{group_col}_txn_{window_name}'\n",
    "\n",
    "        print(f\"Calulcating {col_name}...\")\n",
    "\n",
    "        # Use rolling window\n",
    "        df[col_name] = df.groupby(group_col)[time_col].transform(\n",
    "            lambda x: x.rolling(window=len(x), min_periods=1).apply(\n",
    "                lambda times: ((time.iloc[-1] - times) <= window).sum() - 1,\n",
    "                raw=False\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    print(f\"\\nVelocity features created!\")\n",
    "    return df\n",
    "\n",
    "def engineer_time_features(df: pd.DataFrame, time_col: str = 'TransactionDT') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Engineer time-based features from transaction timestamp.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with time column\n",
    "        time_col: Name of time column (seconds since reference)\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with time features added\n",
    "    \"\"\"\n",
    "    print(f\"\\nEngineering Time Features\\n\")\n",
    "\n",
    "    df[f'{time_col}_hour'] = (df[time_col] // 3600 )% 24\n",
    "    df[f'{time_col}_day'] = (df[time_col] // 86400 )% 7\n",
    "    df[f'{time_col}_is_weekend'] = df[f'{time_col}_day'].isin([5,6]).astype(int)\n",
    "\n",
    "    print(f'Created: {time_col}_hour, {time_col}_day, {time_col}_is_weekend features')\n",
    "    return df\n",
    "\n",
    "def engineer_device_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Engineer device reuse features.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with DeviceInfo column\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with device features added\n",
    "    \"\"\"\n",
    "    print(f\"\\nEngineering Device Features\\n\")\n",
    "    \n",
    "    if 'DeviceInfo' not in df.columns:\n",
    "        print(f\"[WARNING] DeviceInfo not found. Skipping device features.\")\n",
    "        return df\n",
    "\n",
    "    # Cards per device\n",
    "    device_card_counts = df.groupby('DeviceInfo')['card1'].nunique().to_dict()\n",
    "    df['device_card_count'] = df['DeviceInfo'].map(device_card_counts)\n",
    "\n",
    "    # Multi-card device flag\n",
    "    df['device_multi_card'] = (df['device_card_count'] > 1).astype(int)\n",
    "\n",
    "    print(f\"Created: device_card_count, device_multi_card features\")\n",
    "    return df\n",
    "\n",
    "def engineer_amount_features(df: pd.DataFrame, amount_col: str = 'TransactionAmt') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Engineer amount-based statistical features.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with amount column\n",
    "        amount_col: Name of amount column\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with amount features added\n",
    "    \"\"\"\n",
    "    print(f\"\\nEngineering Amount Features\\n\")\n",
    "\n",
    "    if 'card1' not in df.columns:\n",
    "        print(f\"[WARNING] card1 not found. Skipping amount features.\")\n",
    "        return df\n",
    "    \n",
    "    # Per-card statistics\n",
    "    cards_stats = df.groupby('card1')[amount_col].agg(['mean', 'std']).reset_index()\n",
    "    cards_stats.columns = ['card1', 'card_amt_mean', 'card_amt_std']\n",
    "\n",
    "    df = df.merge(cards_stats, on='card1', how='left')\n",
    "    \n",
    "\n",
    "    # Z-score\n",
    "    df['amt_zscore'] = (df[amount_col] - df['card_amt_mean']) / (df['card_amt_std'] + 1e-6)\n",
    "\n",
    "    df['amt_zscore'].fillna(0, inplace=True)\n",
    "    \n",
    "    print(f\"Created: card_amt_mean, card_amt_std, amt_zscore features\")\n",
    "    return df\n",
    "\n",
    "print('\\nHelper functions loaded')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe7d99b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
